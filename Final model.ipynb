{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project uses a text convolutional neural network and uses the MovieLens dataset to complete the task of movie recommendation.\n",
    "\n",
    "This project uses the MovieLens 1M dataset, which contains 100 million comments on nearly 4000 movies from 6000 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.clear_all_output();\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.clear_all_output();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda3\\envs\\665final\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "E:\\anaconda3\\envs\\665final\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "E:\\anaconda3\\envs\\665final\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "E:\\anaconda3\\envs\\665final\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "E:\\anaconda3\\envs\\665final\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "E:\\anaconda3\\envs\\665final\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "E:\\anaconda3\\envs\\665final\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "E:\\anaconda3\\envs\\665final\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "E:\\anaconda3\\envs\\665final\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "E:\\anaconda3\\envs\\665final\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "E:\\anaconda3\\envs\\665final\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "E:\\anaconda3\\envs\\665final\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import tensorflow as tf\n",
    "#import tensorflow.compat.v1 as tf\n",
    "# from tensorflow.python.framework import ops\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View data\n",
    "\n",
    "The data set is divided into three files: user data users.dat, movie data movies.dat and rating data ratings.dat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>OccupationID</th>\n",
       "      <th>Zip-code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID Gender  Age  OccupationID Zip-code\n",
       "0       1      F    1            10    48067\n",
       "1       2      M   56            16    70072\n",
       "2       3      M   25            15    55117\n",
       "3       4      M   45             7    02460\n",
       "4       5      M   25            20    55455"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_title = ['UserID', 'Gender', 'Age', 'OccupationID', 'Zip-code']\n",
    "users = pd.read_table('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                               Title                        Genres\n",
       "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4        5  Father of the Bride Part II (1995)                        Comedy"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_title = ['MovieID', 'Title', 'Genres']\n",
    "movies = pd.read_table('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating  timestamps\n",
       "0       1     1193       5   978300760\n",
       "1       1      661       3   978302109\n",
       "2       1      914       3   978301968\n",
       "3       1     3408       4   978300275\n",
       "4       1     2355       5   978824291"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_title = ['UserID','MovieID', 'Rating', 'timestamps']\n",
    "ratings = pd.read_table('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UserID, Occupation and MovieID remain unchanged.\n",
    "\n",
    "Gender field: Need to convert ‘F’ and ‘M’ to 0 and 1.\n",
    "\n",
    "Age field: to be converted into 7 consecutive numbers 0 to 6.\n",
    "\n",
    "Genres field: It is a classification field and needs to be converted into a number. First convert the categories in Genres into a dictionary of strings to numbers, and then convert the Genres field of each movie into a list of numbers, because some movies are a combination of multiple Genres.\n",
    "\n",
    "Title field: The processing method is the same as the Genres field. First, create a text-to-number dictionary, and then convert the description in the Title into a list of numbers. In addition, the year in the Title also needs to be removed.\n",
    "\n",
    "The Genres and Title fields need to be uniform in length so that they can be easily processed in the neural network. The blank part is filled with the number corresponding to ‘< PAD >’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_function import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and save to local\n",
    "\n",
    "title_count: Length of Title field (15)\n",
    "\n",
    "title_set: a collection of Title text\n",
    "\n",
    "genres2int: A dictionary of movie genres to numbers\n",
    "\n",
    "features: is input X\n",
    "\n",
    "targets_values: is the learning target y\n",
    "\n",
    "ratings: Pandas object of the rating data set\n",
    "\n",
    "users: Pandas object of the user data set\n",
    "\n",
    "movies: Pandas object of movie data\n",
    "\n",
    "data: Pandas object composed of three data sets\n",
    "\n",
    "movies_orig: Original movie data without data processing\n",
    "\n",
    "users_orig: Original user data without data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = load_data()\n",
    "\n",
    "pickle.dump((title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>JobID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  Gender  Age  JobID\n",
       "0       1       0    0     10\n",
       "1       2       1    5     16\n",
       "2       3       1    6     15\n",
       "3       4       1    2      7\n",
       "4       5       1    6     20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[1803, 1164, 4647, 4647, 4647, 4647, 4647, 464...</td>\n",
       "      <td>[14, 18, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[682, 4647, 4647, 4647, 4647, 4647, 4647, 4647...</td>\n",
       "      <td>[16, 18, 15, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[3119, 1395, 1714, 4647, 4647, 4647, 4647, 464...</td>\n",
       "      <td>[2, 9, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[3115, 3591, 670, 4647, 4647, 4647, 4647, 4647...</td>\n",
       "      <td>[2, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[3869, 4080, 5139, 2397, 4029, 4291, 4647, 464...</td>\n",
       "      <td>[2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                                              Title  \\\n",
       "0        1  [1803, 1164, 4647, 4647, 4647, 4647, 4647, 464...   \n",
       "1        2  [682, 4647, 4647, 4647, 4647, 4647, 4647, 4647...   \n",
       "2        3  [3119, 1395, 1714, 4647, 4647, 4647, 4647, 464...   \n",
       "3        4  [3115, 3591, 670, 4647, 4647, 4647, 4647, 4647...   \n",
       "4        5  [3869, 4080, 5139, 2397, 4029, 4291, 4647, 464...   \n",
       "\n",
       "                                              Genres  \n",
       "0  [14, 18, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4...  \n",
       "1  [16, 18, 15, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
       "2  [2, 9, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
       "3  [2, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
       "4  [2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1,\n",
       "       list([1803, 1164, 4647, 4647, 4647, 4647, 4647, 4647, 4647, 4647, 4647, 4647, 4647, 4647, 4647]),\n",
       "       list([14, 18, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from local\n",
    "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = pickle.load(open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wish to remember the params what we used.\n",
    "def save_params(params): #Save parameters to file\n",
    "    pickle.dump(params, open('params.p', 'wb'))\n",
    "\n",
    "def load_params(): #Load parameters from file\n",
    "    return pickle.load(open('params.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "\n",
    "# Dimensions of the embedding matrix\n",
    "embed_dim = 32\n",
    "# Number of user IDs\n",
    "uid_max = max(features.take(0,1)) + 1 # 6040\n",
    "# Number of sex\n",
    "gender_max = max(features.take(2,1)) + 1 # 1 + 1 = 2\n",
    "# Number of age categories\n",
    "age_max = max(features.take(3,1)) + 1 # 6 + 1 = 7\n",
    "# Number of occupations\n",
    "job_max = max(features.take(4,1)) + 1# 20 + 1 = 21\n",
    "\n",
    "# Number of movie IDs\n",
    "movie_id_max = max(features.take(1,1)) + 1 # 3952\n",
    "# Number of movie types\n",
    "movie_categories_max = max(genres2int.values()) + 1 # 18 + 1 = 19\n",
    "# Number of movie name words\n",
    "movie_title_max = len(title_set) # 5216\n",
    "\n",
    "# The sign of adding and operating the movie type embedding vector\n",
    "# It seems better to use mean to do average, but this function is not implemented\n",
    "combiner = \"sum\"\n",
    "\n",
    "# Movie name length\n",
    "sentences_size = title_count # = 15\n",
    "# Text convolution sliding window, sliding 2, 3, 4, 5 words respectively\n",
    "window_sizes = {2, 3, 4, 5}\n",
    "# Number of text convolution kernels\n",
    "filter_num = 8\n",
    "\n",
    "# The movie ID is transferred to the subscript dictionary. The movie ID in the data set is inconsistent with the subscript.\n",
    "# For example, the data movie ID in row 5 is not necessarily 5\n",
    "movieid2idx = {val[0]:i for i, val in enumerate(movies.values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Super parameters\n",
    "\n",
    "# Number of Epochs\n",
    "num_epochs = 5\n",
    "# Batch Size\n",
    "batch_size = 256\n",
    "\n",
    "dropout_keep = 0.5\n",
    "# Learning Rate\n",
    "learning_rate = 0.0001\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 20\n",
    "\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Input\n",
    "from models_function import get_inputs \n",
    "\n",
    "####\n",
    "from models_function import get_user_embedding\n",
    "from models_function import get_user_feature_layer\n",
    "from models_function import get_movie_id_embed_layer\n",
    "from models_function import get_movie_categories_layers\n",
    "from models_function import get_movie_cnn_layer\n",
    "from models_function import get_movie_feature_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\renshangsi\\Desktop\\CS665\\Final\\Deep-Learning-Group-10\\models_function.py:148: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From E:\\anaconda3\\envs\\665final\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A2374336C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A2374336C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A2374336C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A2374336C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A237431708>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A237431708>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A237431708>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A237431708>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A237431708>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A237431708>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A237431708>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A237431708>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A237431708>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A237431708>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A237431708>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A237431708>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A2373B6D48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A2373B6D48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A2373B6D48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A2373B6D48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From C:\\Users\\renshangsi\\Desktop\\CS665\\Final\\Deep-Learning-Group-10\\models_function.py:164: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\renshangsi\\Desktop\\CS665\\Final\\Deep-Learning-Group-10\\models_function.py:175: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\renshangsi\\Desktop\\CS665\\Final\\Deep-Learning-Group-10\\models_function.py:191: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\renshangsi\\Desktop\\CS665\\Final\\Deep-Learning-Group-10\\models_function.py:197: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\renshangsi\\Desktop\\CS665\\Final\\Deep-Learning-Group-10\\models_function.py:206: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A23B6C1CC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A23B6C1CC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A23B6C1CC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A23B6C1CC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A23B6C1CC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A23B6C1CC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A23B6C1CC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A23B6C1CC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A237EBEF88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A237EBEF88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A237EBEF88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A237EBEF88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From E:\\anaconda3\\envs\\665final\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    # Get input placeholder\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob = get_inputs()\n",
    "    # Get the 4 embedding vectors of User\n",
    "    uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer = get_user_embedding(uid, user_gender, user_age, user_job, uid_max, embed_dim, gender_max, age_max, job_max)\n",
    "    # Get user characteristics\n",
    "    user_combine_layer, user_combine_layer_flat = get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer, embed_dim)\n",
    "    # Get the embedding vector of movie ID\n",
    "    movie_id_embed_layer = get_movie_id_embed_layer(movie_id, embed_dim, movie_id_max)\n",
    "    # Get the embedding vector of movie type\n",
    "    movie_categories_embed_layer = get_movie_categories_layers(movie_categories, movie_categories_max, embed_dim, combiner)\n",
    "    # Get feature vector of movie name\n",
    "    pool_layer_flat, dropout_layer = get_movie_cnn_layer(movie_titles, movie_title_max, embed_dim, filter_num, window_sizes, sentences_size, dropout_keep_prob)\n",
    "    # Get movie features\n",
    "    movie_combine_layer, movie_combine_layer_flat = get_movie_feature_layer(movie_id_embed_layer, movie_categories_embed_layer, dropout_layer, embed_dim)\n",
    "    \n",
    "    \n",
    "    # Calculate the score\n",
    "    with tf.name_scope(\"inference\"):\n",
    "        # Simply do a matrix multiplication of user features and movie features to get a predicted score\n",
    "        inference = tf.reduce_sum(user_combine_layer_flat * movie_combine_layer_flat, axis=1)\n",
    "        inference = tf.expand_dims(inference, axis=1)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # MSE loss, return the calculated value to the score\n",
    "        cost = tf.losses.mean_squared_error(targets, inference )\n",
    "        loss = tf.reduce_mean(cost)\n",
    "    \n",
    "    # Optimization loss \n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    gradients = optimizer.compute_gradients(loss)  #cost\n",
    "    train_op = optimizer.apply_gradients(gradients, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'inference/ExpandDims:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to C:\\Users\\renshangsi\\Desktop\\CS665\\Final\\Deep-Learning-Group-10\\runs\\1618993576\n",
      "\n",
      "2021-04-21T03:26:18.856848: Epoch   0 Batch    0/3125   train_loss = 23.853\n",
      "2021-04-21T03:26:19.271851: Epoch   0 Batch   20/3125   train_loss = 5.124\n",
      "2021-04-21T03:26:19.688500: Epoch   0 Batch   40/3125   train_loss = 3.400\n",
      "2021-04-21T03:26:20.080577: Epoch   0 Batch   60/3125   train_loss = 2.538\n",
      "2021-04-21T03:26:20.485906: Epoch   0 Batch   80/3125   train_loss = 2.474\n",
      "2021-04-21T03:26:20.877845: Epoch   0 Batch  100/3125   train_loss = 2.234\n",
      "2021-04-21T03:26:21.263793: Epoch   0 Batch  120/3125   train_loss = 1.903\n",
      "2021-04-21T03:26:21.667686: Epoch   0 Batch  140/3125   train_loss = 1.897\n",
      "2021-04-21T03:26:22.076247: Epoch   0 Batch  160/3125   train_loss = 1.617\n",
      "2021-04-21T03:26:22.485172: Epoch   0 Batch  180/3125   train_loss = 1.614\n",
      "2021-04-21T03:26:22.922971: Epoch   0 Batch  200/3125   train_loss = 1.633\n",
      "2021-04-21T03:26:23.326641: Epoch   0 Batch  220/3125   train_loss = 1.465\n",
      "2021-04-21T03:26:23.747445: Epoch   0 Batch  240/3125   train_loss = 1.425\n",
      "2021-04-21T03:26:24.165303: Epoch   0 Batch  260/3125   train_loss = 1.630\n",
      "2021-04-21T03:26:24.577262: Epoch   0 Batch  280/3125   train_loss = 1.443\n",
      "2021-04-21T03:26:24.997165: Epoch   0 Batch  300/3125   train_loss = 1.451\n",
      "2021-04-21T03:26:25.387998: Epoch   0 Batch  320/3125   train_loss = 1.613\n",
      "2021-04-21T03:26:25.803870: Epoch   0 Batch  340/3125   train_loss = 1.352\n",
      "2021-04-21T03:26:26.195989: Epoch   0 Batch  360/3125   train_loss = 1.398\n",
      "2021-04-21T03:26:26.652384: Epoch   0 Batch  380/3125   train_loss = 1.403\n",
      "2021-04-21T03:26:27.062307: Epoch   0 Batch  400/3125   train_loss = 1.222\n",
      "2021-04-21T03:26:27.452331: Epoch   0 Batch  420/3125   train_loss = 1.239\n",
      "2021-04-21T03:26:27.903775: Epoch   0 Batch  440/3125   train_loss = 1.396\n",
      "2021-04-21T03:26:28.347347: Epoch   0 Batch  460/3125   train_loss = 1.393\n",
      "2021-04-21T03:26:28.756253: Epoch   0 Batch  480/3125   train_loss = 1.340\n",
      "2021-04-21T03:26:29.196077: Epoch   0 Batch  500/3125   train_loss = 1.093\n",
      "2021-04-21T03:26:29.616950: Epoch   0 Batch  520/3125   train_loss = 1.337\n",
      "2021-04-21T03:26:30.029334: Epoch   0 Batch  540/3125   train_loss = 1.254\n",
      "2021-04-21T03:26:30.419757: Epoch   0 Batch  560/3125   train_loss = 1.480\n",
      "2021-04-21T03:26:30.866485: Epoch   0 Batch  580/3125   train_loss = 1.362\n",
      "2021-04-21T03:26:31.271523: Epoch   0 Batch  600/3125   train_loss = 1.320\n",
      "2021-04-21T03:26:31.662532: Epoch   0 Batch  620/3125   train_loss = 1.446\n",
      "2021-04-21T03:26:32.089391: Epoch   0 Batch  640/3125   train_loss = 1.405\n",
      "2021-04-21T03:26:32.488192: Epoch   0 Batch  660/3125   train_loss = 1.329\n",
      "2021-04-21T03:26:32.909079: Epoch   0 Batch  680/3125   train_loss = 1.161\n",
      "2021-04-21T03:26:33.304014: Epoch   0 Batch  700/3125   train_loss = 1.397\n",
      "2021-04-21T03:26:33.690090: Epoch   0 Batch  720/3125   train_loss = 1.198\n",
      "2021-04-21T03:26:34.077058: Epoch   0 Batch  740/3125   train_loss = 1.305\n",
      "2021-04-21T03:26:34.481004: Epoch   0 Batch  760/3125   train_loss = 1.356\n",
      "2021-04-21T03:26:34.883776: Epoch   0 Batch  780/3125   train_loss = 1.409\n",
      "2021-04-21T03:26:35.278271: Epoch   0 Batch  800/3125   train_loss = 1.306\n",
      "2021-04-21T03:26:35.678664: Epoch   0 Batch  820/3125   train_loss = 1.244\n",
      "2021-04-21T03:26:36.062709: Epoch   0 Batch  840/3125   train_loss = 1.315\n",
      "2021-04-21T03:26:36.496667: Epoch   0 Batch  860/3125   train_loss = 1.173\n",
      "2021-04-21T03:26:36.904169: Epoch   0 Batch  880/3125   train_loss = 1.207\n",
      "2021-04-21T03:26:37.302070: Epoch   0 Batch  900/3125   train_loss = 1.268\n",
      "2021-04-21T03:26:37.707892: Epoch   0 Batch  920/3125   train_loss = 1.223\n",
      "2021-04-21T03:26:38.142550: Epoch   0 Batch  940/3125   train_loss = 1.365\n",
      "2021-04-21T03:26:38.530428: Epoch   0 Batch  960/3125   train_loss = 1.329\n",
      "2021-04-21T03:26:38.931590: Epoch   0 Batch  980/3125   train_loss = 1.392\n",
      "2021-04-21T03:26:39.326667: Epoch   0 Batch 1000/3125   train_loss = 1.243\n",
      "2021-04-21T03:26:39.715693: Epoch   0 Batch 1020/3125   train_loss = 1.274\n",
      "2021-04-21T03:26:40.107645: Epoch   0 Batch 1040/3125   train_loss = 1.319\n",
      "2021-04-21T03:26:40.484637: Epoch   0 Batch 1060/3125   train_loss = 1.440\n",
      "2021-04-21T03:26:40.868498: Epoch   0 Batch 1080/3125   train_loss = 1.167\n",
      "2021-04-21T03:26:41.263443: Epoch   0 Batch 1100/3125   train_loss = 1.329\n",
      "2021-04-21T03:26:41.649511: Epoch   0 Batch 1120/3125   train_loss = 1.206\n",
      "2021-04-21T03:26:42.084724: Epoch   0 Batch 1140/3125   train_loss = 1.308\n",
      "2021-04-21T03:26:42.482454: Epoch   0 Batch 1160/3125   train_loss = 1.293\n",
      "2021-04-21T03:26:42.863437: Epoch   0 Batch 1180/3125   train_loss = 1.281\n",
      "2021-04-21T03:26:43.259314: Epoch   0 Batch 1200/3125   train_loss = 1.230\n",
      "2021-04-21T03:26:43.637443: Epoch   0 Batch 1220/3125   train_loss = 1.125\n",
      "2021-04-21T03:26:44.013440: Epoch   0 Batch 1240/3125   train_loss = 1.126\n",
      "2021-04-21T03:26:44.403398: Epoch   0 Batch 1260/3125   train_loss = 1.211\n",
      "2021-04-21T03:26:44.806284: Epoch   0 Batch 1280/3125   train_loss = 1.219\n",
      "2021-04-21T03:26:45.218182: Epoch   0 Batch 1300/3125   train_loss = 1.248\n",
      "2021-04-21T03:26:45.602907: Epoch   0 Batch 1320/3125   train_loss = 1.120\n",
      "2021-04-21T03:26:45.997116: Epoch   0 Batch 1340/3125   train_loss = 1.048\n",
      "2021-04-21T03:26:46.417565: Epoch   0 Batch 1360/3125   train_loss = 1.155\n",
      "2021-04-21T03:26:46.852486: Epoch   0 Batch 1380/3125   train_loss = 1.018\n",
      "2021-04-21T03:26:47.245853: Epoch   0 Batch 1400/3125   train_loss = 1.221\n",
      "2021-04-21T03:26:47.634814: Epoch   0 Batch 1420/3125   train_loss = 1.281\n",
      "2021-04-21T03:26:48.070908: Epoch   0 Batch 1440/3125   train_loss = 1.159\n",
      "2021-04-21T03:26:48.465044: Epoch   0 Batch 1460/3125   train_loss = 1.174\n",
      "2021-04-21T03:26:48.852947: Epoch   0 Batch 1480/3125   train_loss = 1.235\n",
      "2021-04-21T03:26:49.235902: Epoch   0 Batch 1500/3125   train_loss = 1.344\n",
      "2021-04-21T03:26:49.628221: Epoch   0 Batch 1520/3125   train_loss = 1.199\n",
      "2021-04-21T03:26:50.019176: Epoch   0 Batch 1540/3125   train_loss = 1.225\n",
      "2021-04-21T03:26:50.417121: Epoch   0 Batch 1560/3125   train_loss = 1.155\n",
      "2021-04-21T03:26:50.812563: Epoch   0 Batch 1580/3125   train_loss = 1.189\n",
      "2021-04-21T03:26:51.205408: Epoch   0 Batch 1600/3125   train_loss = 1.274\n",
      "2021-04-21T03:26:51.592476: Epoch   0 Batch 1620/3125   train_loss = 1.224\n",
      "2021-04-21T03:26:51.995526: Epoch   0 Batch 1640/3125   train_loss = 1.310\n",
      "2021-04-21T03:26:52.390470: Epoch   0 Batch 1660/3125   train_loss = 1.347\n",
      "2021-04-21T03:26:52.773447: Epoch   0 Batch 1680/3125   train_loss = 1.232\n",
      "2021-04-21T03:26:53.166971: Epoch   0 Batch 1700/3125   train_loss = 1.071\n",
      "2021-04-21T03:26:53.562814: Epoch   0 Batch 1720/3125   train_loss = 1.150\n",
      "2021-04-21T03:26:53.960463: Epoch   0 Batch 1740/3125   train_loss = 1.192\n",
      "2021-04-21T03:26:54.355345: Epoch   0 Batch 1760/3125   train_loss = 1.283\n",
      "2021-04-21T03:26:54.766195: Epoch   0 Batch 1780/3125   train_loss = 1.141\n",
      "2021-04-21T03:26:55.170348: Epoch   0 Batch 1800/3125   train_loss = 1.218\n",
      "2021-04-21T03:26:55.579418: Epoch   0 Batch 1820/3125   train_loss = 1.136\n",
      "2021-04-21T03:26:55.972388: Epoch   0 Batch 1840/3125   train_loss = 1.347\n",
      "2021-04-21T03:26:56.368341: Epoch   0 Batch 1860/3125   train_loss = 1.224\n",
      "2021-04-21T03:26:56.792180: Epoch   0 Batch 1880/3125   train_loss = 1.303\n",
      "2021-04-21T03:26:57.177181: Epoch   0 Batch 1900/3125   train_loss = 1.131\n",
      "2021-04-21T03:26:57.568503: Epoch   0 Batch 1920/3125   train_loss = 1.215\n",
      "2021-04-21T03:26:57.992942: Epoch   0 Batch 1940/3125   train_loss = 1.059\n",
      "2021-04-21T03:26:58.382924: Epoch   0 Batch 1960/3125   train_loss = 1.141\n",
      "2021-04-21T03:26:58.763905: Epoch   0 Batch 1980/3125   train_loss = 1.159\n",
      "2021-04-21T03:26:59.157815: Epoch   0 Batch 2000/3125   train_loss = 1.428\n",
      "2021-04-21T03:26:59.587662: Epoch   0 Batch 2020/3125   train_loss = 1.264\n",
      "2021-04-21T03:27:00.007552: Epoch   0 Batch 2040/3125   train_loss = 1.166\n",
      "2021-04-21T03:27:00.412542: Epoch   0 Batch 2060/3125   train_loss = 1.065\n",
      "2021-04-21T03:27:00.806426: Epoch   0 Batch 2080/3125   train_loss = 1.299\n",
      "2021-04-21T03:27:01.229213: Epoch   0 Batch 2100/3125   train_loss = 1.166\n",
      "2021-04-21T03:27:01.620447: Epoch   0 Batch 2120/3125   train_loss = 1.042\n",
      "2021-04-21T03:27:02.049299: Epoch   0 Batch 2140/3125   train_loss = 1.170\n",
      "2021-04-21T03:27:02.433274: Epoch   0 Batch 2160/3125   train_loss = 1.173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-21T03:27:02.844097: Epoch   0 Batch 2180/3125   train_loss = 1.150\n",
      "2021-04-21T03:27:03.259015: Epoch   0 Batch 2200/3125   train_loss = 1.137\n",
      "2021-04-21T03:27:03.647308: Epoch   0 Batch 2220/3125   train_loss = 1.124\n",
      "2021-04-21T03:27:04.063782: Epoch   0 Batch 2240/3125   train_loss = 1.112\n",
      "2021-04-21T03:27:04.480689: Epoch   0 Batch 2260/3125   train_loss = 1.149\n",
      "2021-04-21T03:27:04.896671: Epoch   0 Batch 2280/3125   train_loss = 1.171\n",
      "2021-04-21T03:27:05.331414: Epoch   0 Batch 2300/3125   train_loss = 1.171\n",
      "2021-04-21T03:27:05.717383: Epoch   0 Batch 2320/3125   train_loss = 1.324\n",
      "2021-04-21T03:27:06.159108: Epoch   0 Batch 2340/3125   train_loss = 1.217\n",
      "2021-04-21T03:27:06.634809: Epoch   0 Batch 2360/3125   train_loss = 1.154\n",
      "2021-04-21T03:27:07.064472: Epoch   0 Batch 2380/3125   train_loss = 1.098\n",
      "2021-04-21T03:27:07.498806: Epoch   0 Batch 2400/3125   train_loss = 1.283\n",
      "2021-04-21T03:27:07.902745: Epoch   0 Batch 2420/3125   train_loss = 1.147\n",
      "2021-04-21T03:27:08.338579: Epoch   0 Batch 2440/3125   train_loss = 1.284\n",
      "2021-04-21T03:27:08.741607: Epoch   0 Batch 2460/3125   train_loss = 1.126\n",
      "2021-04-21T03:27:09.164456: Epoch   0 Batch 2480/3125   train_loss = 1.208\n",
      "2021-04-21T03:27:09.585359: Epoch   0 Batch 2500/3125   train_loss = 1.159\n",
      "2021-04-21T03:27:10.041042: Epoch   0 Batch 2520/3125   train_loss = 1.106\n",
      "2021-04-21T03:27:10.461320: Epoch   0 Batch 2540/3125   train_loss = 1.025\n",
      "2021-04-21T03:27:10.874633: Epoch   0 Batch 2560/3125   train_loss = 0.949\n",
      "2021-04-21T03:27:11.268782: Epoch   0 Batch 2580/3125   train_loss = 1.065\n",
      "2021-04-21T03:27:11.670690: Epoch   0 Batch 2600/3125   train_loss = 1.184\n",
      "2021-04-21T03:27:12.064876: Epoch   0 Batch 2620/3125   train_loss = 1.044\n",
      "2021-04-21T03:27:12.461602: Epoch   0 Batch 2640/3125   train_loss = 1.098\n",
      "2021-04-21T03:27:12.853139: Epoch   0 Batch 2660/3125   train_loss = 1.214\n",
      "2021-04-21T03:27:13.250946: Epoch   0 Batch 2680/3125   train_loss = 1.054\n",
      "2021-04-21T03:27:13.652093: Epoch   0 Batch 2700/3125   train_loss = 1.142\n",
      "2021-04-21T03:27:14.056018: Epoch   0 Batch 2720/3125   train_loss = 1.146\n",
      "2021-04-21T03:27:14.453553: Epoch   0 Batch 2740/3125   train_loss = 1.190\n",
      "2021-04-21T03:27:14.856202: Epoch   0 Batch 2760/3125   train_loss = 1.180\n",
      "2021-04-21T03:27:15.268127: Epoch   0 Batch 2780/3125   train_loss = 1.095\n",
      "2021-04-21T03:27:15.682296: Epoch   0 Batch 2800/3125   train_loss = 1.429\n",
      "2021-04-21T03:27:16.082925: Epoch   0 Batch 2820/3125   train_loss = 1.367\n",
      "2021-04-21T03:27:16.478892: Epoch   0 Batch 2840/3125   train_loss = 1.167\n",
      "2021-04-21T03:27:16.939479: Epoch   0 Batch 2860/3125   train_loss = 1.141\n",
      "2021-04-21T03:27:17.332703: Epoch   0 Batch 2880/3125   train_loss = 1.161\n",
      "2021-04-21T03:27:17.723741: Epoch   0 Batch 2900/3125   train_loss = 1.230\n",
      "2021-04-21T03:27:18.158333: Epoch   0 Batch 2920/3125   train_loss = 1.229\n",
      "2021-04-21T03:27:18.556406: Epoch   0 Batch 2940/3125   train_loss = 1.161\n",
      "2021-04-21T03:27:18.961333: Epoch   0 Batch 2960/3125   train_loss = 1.111\n",
      "2021-04-21T03:27:19.356704: Epoch   0 Batch 2980/3125   train_loss = 1.174\n",
      "2021-04-21T03:27:19.763580: Epoch   0 Batch 3000/3125   train_loss = 1.165\n",
      "2021-04-21T03:27:20.168626: Epoch   0 Batch 3020/3125   train_loss = 1.206\n",
      "2021-04-21T03:27:20.564954: Epoch   0 Batch 3040/3125   train_loss = 1.146\n",
      "2021-04-21T03:27:20.959707: Epoch   0 Batch 3060/3125   train_loss = 1.174\n",
      "2021-04-21T03:27:21.367876: Epoch   0 Batch 3080/3125   train_loss = 1.213\n",
      "2021-04-21T03:27:21.772700: Epoch   0 Batch 3100/3125   train_loss = 1.171\n",
      "2021-04-21T03:27:22.181494: Epoch   0 Batch 3120/3125   train_loss = 1.068\n",
      "2021-04-21T03:27:22.455275: Epoch   0 Batch    0/781   test_loss = 0.962\n",
      "2021-04-21T03:27:22.574208: Epoch   0 Batch   20/781   test_loss = 1.132\n",
      "2021-04-21T03:27:22.692074: Epoch   0 Batch   40/781   test_loss = 1.072\n",
      "2021-04-21T03:27:22.809220: Epoch   0 Batch   60/781   test_loss = 1.339\n",
      "2021-04-21T03:27:22.920222: Epoch   0 Batch   80/781   test_loss = 1.313\n",
      "2021-04-21T03:27:23.045064: Epoch   0 Batch  100/781   test_loss = 1.311\n",
      "2021-04-21T03:27:23.170221: Epoch   0 Batch  120/781   test_loss = 1.250\n",
      "2021-04-21T03:27:23.287425: Epoch   0 Batch  140/781   test_loss = 1.173\n",
      "2021-04-21T03:27:23.404760: Epoch   0 Batch  160/781   test_loss = 1.395\n",
      "2021-04-21T03:27:23.517506: Epoch   0 Batch  180/781   test_loss = 1.202\n",
      "2021-04-21T03:27:23.635097: Epoch   0 Batch  200/781   test_loss = 1.226\n",
      "2021-04-21T03:27:23.745933: Epoch   0 Batch  220/781   test_loss = 0.980\n",
      "2021-04-21T03:27:23.861055: Epoch   0 Batch  240/781   test_loss = 1.169\n",
      "2021-04-21T03:27:23.973717: Epoch   0 Batch  260/781   test_loss = 1.133\n",
      "2021-04-21T03:27:24.086021: Epoch   0 Batch  280/781   test_loss = 1.417\n",
      "2021-04-21T03:27:24.199404: Epoch   0 Batch  300/781   test_loss = 1.177\n",
      "2021-04-21T03:27:24.312734: Epoch   0 Batch  320/781   test_loss = 1.392\n",
      "2021-04-21T03:27:24.432233: Epoch   0 Batch  340/781   test_loss = 0.880\n",
      "2021-04-21T03:27:24.548760: Epoch   0 Batch  360/781   test_loss = 1.281\n",
      "2021-04-21T03:27:24.676517: Epoch   0 Batch  380/781   test_loss = 1.139\n",
      "2021-04-21T03:27:24.795945: Epoch   0 Batch  400/781   test_loss = 1.087\n",
      "2021-04-21T03:27:24.930005: Epoch   0 Batch  420/781   test_loss = 1.040\n",
      "2021-04-21T03:27:25.048466: Epoch   0 Batch  440/781   test_loss = 1.252\n",
      "2021-04-21T03:27:25.158887: Epoch   0 Batch  460/781   test_loss = 1.105\n",
      "2021-04-21T03:27:25.275790: Epoch   0 Batch  480/781   test_loss = 1.085\n",
      "2021-04-21T03:27:25.386564: Epoch   0 Batch  500/781   test_loss = 0.990\n",
      "2021-04-21T03:27:25.500870: Epoch   0 Batch  520/781   test_loss = 1.201\n",
      "2021-04-21T03:27:25.616597: Epoch   0 Batch  540/781   test_loss = 1.065\n",
      "2021-04-21T03:27:25.739191: Epoch   0 Batch  560/781   test_loss = 1.307\n",
      "2021-04-21T03:27:25.852914: Epoch   0 Batch  580/781   test_loss = 1.154\n",
      "2021-04-21T03:27:25.969846: Epoch   0 Batch  600/781   test_loss = 1.160\n",
      "2021-04-21T03:27:26.086685: Epoch   0 Batch  620/781   test_loss = 1.163\n",
      "2021-04-21T03:27:26.204785: Epoch   0 Batch  640/781   test_loss = 1.231\n",
      "2021-04-21T03:27:26.323097: Epoch   0 Batch  660/781   test_loss = 1.170\n",
      "2021-04-21T03:27:26.441052: Epoch   0 Batch  680/781   test_loss = 1.461\n",
      "2021-04-21T03:27:26.580374: Epoch   0 Batch  700/781   test_loss = 1.117\n",
      "2021-04-21T03:27:26.698133: Epoch   0 Batch  720/781   test_loss = 1.303\n",
      "2021-04-21T03:27:26.868092: Epoch   0 Batch  740/781   test_loss = 1.155\n",
      "2021-04-21T03:27:26.987388: Epoch   0 Batch  760/781   test_loss = 1.151\n",
      "2021-04-21T03:27:27.100858: Epoch   0 Batch  780/781   test_loss = 1.199\n",
      "2021-04-21T03:27:28.106009: Epoch   1 Batch   15/3125   train_loss = 1.218\n",
      "2021-04-21T03:27:28.507563: Epoch   1 Batch   35/3125   train_loss = 1.176\n",
      "2021-04-21T03:27:28.915409: Epoch   1 Batch   55/3125   train_loss = 1.258\n",
      "2021-04-21T03:27:29.308819: Epoch   1 Batch   75/3125   train_loss = 1.125\n",
      "2021-04-21T03:27:29.690758: Epoch   1 Batch   95/3125   train_loss = 1.002\n",
      "2021-04-21T03:27:30.086063: Epoch   1 Batch  115/3125   train_loss = 1.196\n",
      "2021-04-21T03:27:30.482981: Epoch   1 Batch  135/3125   train_loss = 0.981\n",
      "2021-04-21T03:27:30.863987: Epoch   1 Batch  155/3125   train_loss = 1.126\n",
      "2021-04-21T03:27:31.253948: Epoch   1 Batch  175/3125   train_loss = 1.103\n",
      "2021-04-21T03:27:31.643631: Epoch   1 Batch  195/3125   train_loss = 1.193\n",
      "2021-04-21T03:27:32.041766: Epoch   1 Batch  215/3125   train_loss = 1.171\n",
      "2021-04-21T03:27:32.431744: Epoch   1 Batch  235/3125   train_loss = 1.059\n",
      "2021-04-21T03:27:32.827192: Epoch   1 Batch  255/3125   train_loss = 1.228\n",
      "2021-04-21T03:27:33.210083: Epoch   1 Batch  275/3125   train_loss = 1.048\n",
      "2021-04-21T03:27:33.610021: Epoch   1 Batch  295/3125   train_loss = 1.020\n",
      "2021-04-21T03:27:34.021694: Epoch   1 Batch  315/3125   train_loss = 1.069\n",
      "2021-04-21T03:27:34.425247: Epoch   1 Batch  335/3125   train_loss = 1.003\n",
      "2021-04-21T03:27:34.813893: Epoch   1 Batch  355/3125   train_loss = 1.160\n",
      "2021-04-21T03:27:35.232856: Epoch   1 Batch  375/3125   train_loss = 1.160\n",
      "2021-04-21T03:27:35.635776: Epoch   1 Batch  395/3125   train_loss = 1.010\n",
      "2021-04-21T03:27:36.023735: Epoch   1 Batch  415/3125   train_loss = 1.254\n",
      "2021-04-21T03:27:36.414538: Epoch   1 Batch  435/3125   train_loss = 1.128\n",
      "2021-04-21T03:27:36.818484: Epoch   1 Batch  455/3125   train_loss = 1.008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-21T03:27:37.247354: Epoch   1 Batch  475/3125   train_loss = 1.161\n",
      "2021-04-21T03:27:37.649280: Epoch   1 Batch  495/3125   train_loss = 1.075\n",
      "2021-04-21T03:27:38.052203: Epoch   1 Batch  515/3125   train_loss = 1.165\n",
      "2021-04-21T03:27:38.476271: Epoch   1 Batch  535/3125   train_loss = 1.177\n",
      "2021-04-21T03:27:38.875570: Epoch   1 Batch  555/3125   train_loss = 1.230\n",
      "2021-04-21T03:27:39.273691: Epoch   1 Batch  575/3125   train_loss = 1.123\n",
      "2021-04-21T03:27:39.670031: Epoch   1 Batch  595/3125   train_loss = 1.269\n",
      "2021-04-21T03:27:40.049024: Epoch   1 Batch  615/3125   train_loss = 1.096\n",
      "2021-04-21T03:27:40.452000: Epoch   1 Batch  635/3125   train_loss = 1.215\n",
      "2021-04-21T03:27:40.843944: Epoch   1 Batch  655/3125   train_loss = 1.002\n",
      "2021-04-21T03:27:41.244301: Epoch   1 Batch  675/3125   train_loss = 0.929\n",
      "2021-04-21T03:27:41.649233: Epoch   1 Batch  695/3125   train_loss = 1.011\n",
      "2021-04-21T03:27:42.032964: Epoch   1 Batch  715/3125   train_loss = 1.175\n",
      "2021-04-21T03:27:42.433892: Epoch   1 Batch  735/3125   train_loss = 0.963\n",
      "2021-04-21T03:27:42.827544: Epoch   1 Batch  755/3125   train_loss = 1.195\n",
      "2021-04-21T03:27:43.232465: Epoch   1 Batch  775/3125   train_loss = 0.983\n",
      "2021-04-21T03:27:43.627395: Epoch   1 Batch  795/3125   train_loss = 1.231\n",
      "2021-04-21T03:27:44.038266: Epoch   1 Batch  815/3125   train_loss = 1.088\n",
      "2021-04-21T03:27:44.436253: Epoch   1 Batch  835/3125   train_loss = 1.058\n",
      "2021-04-21T03:27:44.850110: Epoch   1 Batch  855/3125   train_loss = 1.324\n",
      "2021-04-21T03:27:45.251292: Epoch   1 Batch  875/3125   train_loss = 1.164\n",
      "2021-04-21T03:27:45.648546: Epoch   1 Batch  895/3125   train_loss = 1.049\n",
      "2021-04-21T03:27:46.038762: Epoch   1 Batch  915/3125   train_loss = 1.093\n",
      "2021-04-21T03:27:46.434401: Epoch   1 Batch  935/3125   train_loss = 1.288\n",
      "2021-04-21T03:27:46.856311: Epoch   1 Batch  955/3125   train_loss = 1.159\n",
      "2021-04-21T03:27:47.310123: Epoch   1 Batch  975/3125   train_loss = 1.065\n",
      "2021-04-21T03:27:47.703804: Epoch   1 Batch  995/3125   train_loss = 0.885\n",
      "2021-04-21T03:27:48.100025: Epoch   1 Batch 1015/3125   train_loss = 1.121\n",
      "2021-04-21T03:27:48.538654: Epoch   1 Batch 1035/3125   train_loss = 1.048\n",
      "2021-04-21T03:27:48.932535: Epoch   1 Batch 1055/3125   train_loss = 1.120\n",
      "2021-04-21T03:27:49.350419: Epoch   1 Batch 1075/3125   train_loss = 1.028\n",
      "2021-04-21T03:27:49.751197: Epoch   1 Batch 1095/3125   train_loss = 0.983\n",
      "2021-04-21T03:27:50.144155: Epoch   1 Batch 1115/3125   train_loss = 1.165\n",
      "2021-04-21T03:27:50.539795: Epoch   1 Batch 1135/3125   train_loss = 1.039\n",
      "2021-04-21T03:27:50.926734: Epoch   1 Batch 1155/3125   train_loss = 1.083\n",
      "2021-04-21T03:27:51.320890: Epoch   1 Batch 1175/3125   train_loss = 1.150\n",
      "2021-04-21T03:27:51.712440: Epoch   1 Batch 1195/3125   train_loss = 1.273\n",
      "2021-04-21T03:27:52.110769: Epoch   1 Batch 1215/3125   train_loss = 1.003\n",
      "2021-04-21T03:27:52.508605: Epoch   1 Batch 1235/3125   train_loss = 1.096\n",
      "2021-04-21T03:27:52.906516: Epoch   1 Batch 1255/3125   train_loss = 0.977\n",
      "2021-04-21T03:27:53.309399: Epoch   1 Batch 1275/3125   train_loss = 1.048\n",
      "2021-04-21T03:27:53.717370: Epoch   1 Batch 1295/3125   train_loss = 1.078\n",
      "2021-04-21T03:27:54.113312: Epoch   1 Batch 1315/3125   train_loss = 1.226\n",
      "2021-04-21T03:27:54.512436: Epoch   1 Batch 1335/3125   train_loss = 1.016\n",
      "2021-04-21T03:27:54.927973: Epoch   1 Batch 1355/3125   train_loss = 1.072\n",
      "2021-04-21T03:27:55.344330: Epoch   1 Batch 1375/3125   train_loss = 1.095\n",
      "2021-04-21T03:27:55.735283: Epoch   1 Batch 1395/3125   train_loss = 1.146\n",
      "2021-04-21T03:27:56.135222: Epoch   1 Batch 1415/3125   train_loss = 1.128\n",
      "2021-04-21T03:27:56.548414: Epoch   1 Batch 1435/3125   train_loss = 1.208\n",
      "2021-04-21T03:27:56.948318: Epoch   1 Batch 1455/3125   train_loss = 1.191\n",
      "2021-04-21T03:27:57.392372: Epoch   1 Batch 1475/3125   train_loss = 1.164\n",
      "2021-04-21T03:27:57.783311: Epoch   1 Batch 1495/3125   train_loss = 1.023\n",
      "2021-04-21T03:27:58.224214: Epoch   1 Batch 1515/3125   train_loss = 0.949\n",
      "2021-04-21T03:27:58.609591: Epoch   1 Batch 1535/3125   train_loss = 0.896\n",
      "2021-04-21T03:27:59.006509: Epoch   1 Batch 1555/3125   train_loss = 1.076\n",
      "2021-04-21T03:27:59.405472: Epoch   1 Batch 1575/3125   train_loss = 1.041\n",
      "2021-04-21T03:27:59.793434: Epoch   1 Batch 1595/3125   train_loss = 1.106\n",
      "2021-04-21T03:28:00.188340: Epoch   1 Batch 1615/3125   train_loss = 1.070\n",
      "2021-04-21T03:28:00.586317: Epoch   1 Batch 1635/3125   train_loss = 1.189\n",
      "2021-04-21T03:28:00.991886: Epoch   1 Batch 1655/3125   train_loss = 1.186\n",
      "2021-04-21T03:28:01.398839: Epoch   1 Batch 1675/3125   train_loss = 0.968\n",
      "2021-04-21T03:28:01.798796: Epoch   1 Batch 1695/3125   train_loss = 1.060\n",
      "2021-04-21T03:28:02.211693: Epoch   1 Batch 1715/3125   train_loss = 0.985\n",
      "2021-04-21T03:28:02.604955: Epoch   1 Batch 1735/3125   train_loss = 1.147\n",
      "2021-04-21T03:28:03.012848: Epoch   1 Batch 1755/3125   train_loss = 0.959\n",
      "2021-04-21T03:28:03.407680: Epoch   1 Batch 1775/3125   train_loss = 1.036\n",
      "2021-04-21T03:28:03.815543: Epoch   1 Batch 1795/3125   train_loss = 1.149\n",
      "2021-04-21T03:28:04.224941: Epoch   1 Batch 1815/3125   train_loss = 1.020\n",
      "2021-04-21T03:28:04.631431: Epoch   1 Batch 1835/3125   train_loss = 1.118\n",
      "2021-04-21T03:28:05.051168: Epoch   1 Batch 1855/3125   train_loss = 1.028\n",
      "2021-04-21T03:28:05.448132: Epoch   1 Batch 1875/3125   train_loss = 1.145\n",
      "2021-04-21T03:28:05.831117: Epoch   1 Batch 1895/3125   train_loss = 0.983\n",
      "2021-04-21T03:28:06.247274: Epoch   1 Batch 1915/3125   train_loss = 0.877\n",
      "2021-04-21T03:28:06.677091: Epoch   1 Batch 1935/3125   train_loss = 1.011\n",
      "2021-04-21T03:28:07.079791: Epoch   1 Batch 1955/3125   train_loss = 1.031\n",
      "2021-04-21T03:28:07.512420: Epoch   1 Batch 1975/3125   train_loss = 1.034\n",
      "2021-04-21T03:28:07.899207: Epoch   1 Batch 1995/3125   train_loss = 1.187\n",
      "2021-04-21T03:28:08.314351: Epoch   1 Batch 2015/3125   train_loss = 1.171\n",
      "2021-04-21T03:28:08.708334: Epoch   1 Batch 2035/3125   train_loss = 1.152\n",
      "2021-04-21T03:28:09.097973: Epoch   1 Batch 2055/3125   train_loss = 1.003\n",
      "2021-04-21T03:28:09.497993: Epoch   1 Batch 2075/3125   train_loss = 1.206\n",
      "2021-04-21T03:28:09.884175: Epoch   1 Batch 2095/3125   train_loss = 0.926\n",
      "2021-04-21T03:28:10.288096: Epoch   1 Batch 2115/3125   train_loss = 1.207\n",
      "2021-04-21T03:28:10.682118: Epoch   1 Batch 2135/3125   train_loss = 1.003\n",
      "2021-04-21T03:28:11.082050: Epoch   1 Batch 2155/3125   train_loss = 1.039\n",
      "2021-04-21T03:28:11.478855: Epoch   1 Batch 2175/3125   train_loss = 1.072\n",
      "2021-04-21T03:28:11.876351: Epoch   1 Batch 2195/3125   train_loss = 1.089\n",
      "2021-04-21T03:28:12.264489: Epoch   1 Batch 2215/3125   train_loss = 1.107\n",
      "2021-04-21T03:28:12.665216: Epoch   1 Batch 2235/3125   train_loss = 1.139\n",
      "2021-04-21T03:28:13.066774: Epoch   1 Batch 2255/3125   train_loss = 1.109\n",
      "2021-04-21T03:28:13.482299: Epoch   1 Batch 2275/3125   train_loss = 0.928\n",
      "2021-04-21T03:28:13.873711: Epoch   1 Batch 2295/3125   train_loss = 1.235\n",
      "2021-04-21T03:28:14.273646: Epoch   1 Batch 2315/3125   train_loss = 1.181\n",
      "2021-04-21T03:28:14.670586: Epoch   1 Batch 2335/3125   train_loss = 1.034\n",
      "2021-04-21T03:28:15.079637: Epoch   1 Batch 2355/3125   train_loss = 1.078\n",
      "2021-04-21T03:28:15.483582: Epoch   1 Batch 2375/3125   train_loss = 1.282\n",
      "2021-04-21T03:28:15.889541: Epoch   1 Batch 2395/3125   train_loss = 1.048\n",
      "2021-04-21T03:28:16.280437: Epoch   1 Batch 2415/3125   train_loss = 1.074\n",
      "2021-04-21T03:28:16.702931: Epoch   1 Batch 2435/3125   train_loss = 0.993\n",
      "2021-04-21T03:28:17.098884: Epoch   1 Batch 2455/3125   train_loss = 1.134\n",
      "2021-04-21T03:28:17.527737: Epoch   1 Batch 2475/3125   train_loss = 1.033\n",
      "2021-04-21T03:28:17.923987: Epoch   1 Batch 2495/3125   train_loss = 0.985\n",
      "2021-04-21T03:28:18.344649: Epoch   1 Batch 2515/3125   train_loss = 1.117\n",
      "2021-04-21T03:28:18.748543: Epoch   1 Batch 2535/3125   train_loss = 1.063\n",
      "2021-04-21T03:28:19.152951: Epoch   1 Batch 2555/3125   train_loss = 0.913\n",
      "2021-04-21T03:28:19.552188: Epoch   1 Batch 2575/3125   train_loss = 0.921\n",
      "2021-04-21T03:28:19.955227: Epoch   1 Batch 2595/3125   train_loss = 1.025\n",
      "2021-04-21T03:28:20.345379: Epoch   1 Batch 2615/3125   train_loss = 1.134\n",
      "2021-04-21T03:28:20.754262: Epoch   1 Batch 2635/3125   train_loss = 0.994\n",
      "2021-04-21T03:28:21.153605: Epoch   1 Batch 2655/3125   train_loss = 0.990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-21T03:28:21.551145: Epoch   1 Batch 2675/3125   train_loss = 0.990\n",
      "2021-04-21T03:28:21.954445: Epoch   1 Batch 2695/3125   train_loss = 1.117\n",
      "2021-04-21T03:28:22.347027: Epoch   1 Batch 2715/3125   train_loss = 0.985\n",
      "2021-04-21T03:28:22.732997: Epoch   1 Batch 2735/3125   train_loss = 0.869\n",
      "2021-04-21T03:28:23.120020: Epoch   1 Batch 2755/3125   train_loss = 1.187\n",
      "2021-04-21T03:28:23.531934: Epoch   1 Batch 2775/3125   train_loss = 1.015\n",
      "2021-04-21T03:28:23.931805: Epoch   1 Batch 2795/3125   train_loss = 0.992\n",
      "2021-04-21T03:28:24.338580: Epoch   1 Batch 2815/3125   train_loss = 0.941\n",
      "2021-04-21T03:28:24.737234: Epoch   1 Batch 2835/3125   train_loss = 1.054\n",
      "2021-04-21T03:28:25.156460: Epoch   1 Batch 2855/3125   train_loss = 1.048\n",
      "2021-04-21T03:28:25.552383: Epoch   1 Batch 2875/3125   train_loss = 1.039\n",
      "2021-04-21T03:28:25.953428: Epoch   1 Batch 2895/3125   train_loss = 1.080\n",
      "2021-04-21T03:28:26.352348: Epoch   1 Batch 2915/3125   train_loss = 0.942\n",
      "2021-04-21T03:28:26.762834: Epoch   1 Batch 2935/3125   train_loss = 1.112\n",
      "2021-04-21T03:28:27.155204: Epoch   1 Batch 2955/3125   train_loss = 1.133\n",
      "2021-04-21T03:28:27.584033: Epoch   1 Batch 2975/3125   train_loss = 1.012\n",
      "2021-04-21T03:28:27.978951: Epoch   1 Batch 2995/3125   train_loss = 0.987\n",
      "2021-04-21T03:28:28.399723: Epoch   1 Batch 3015/3125   train_loss = 0.972\n",
      "2021-04-21T03:28:28.793636: Epoch   1 Batch 3035/3125   train_loss = 0.974\n",
      "2021-04-21T03:28:29.189707: Epoch   1 Batch 3055/3125   train_loss = 1.051\n",
      "2021-04-21T03:28:29.588640: Epoch   1 Batch 3075/3125   train_loss = 1.033\n",
      "2021-04-21T03:28:29.980408: Epoch   1 Batch 3095/3125   train_loss = 0.975\n",
      "2021-04-21T03:28:30.393278: Epoch   1 Batch 3115/3125   train_loss = 0.830\n",
      "2021-04-21T03:28:30.691899: Epoch   1 Batch   19/781   test_loss = 1.088\n",
      "2021-04-21T03:28:30.806370: Epoch   1 Batch   39/781   test_loss = 0.838\n",
      "2021-04-21T03:28:30.928417: Epoch   1 Batch   59/781   test_loss = 0.948\n",
      "2021-04-21T03:28:31.047975: Epoch   1 Batch   79/781   test_loss = 1.018\n",
      "2021-04-21T03:28:31.164477: Epoch   1 Batch   99/781   test_loss = 1.026\n",
      "2021-04-21T03:28:31.278843: Epoch   1 Batch  119/781   test_loss = 0.901\n",
      "2021-04-21T03:28:31.400100: Epoch   1 Batch  139/781   test_loss = 1.045\n",
      "2021-04-21T03:28:31.515525: Epoch   1 Batch  159/781   test_loss = 1.035\n",
      "2021-04-21T03:28:31.631371: Epoch   1 Batch  179/781   test_loss = 0.930\n",
      "2021-04-21T03:28:31.740581: Epoch   1 Batch  199/781   test_loss = 0.948\n",
      "2021-04-21T03:28:31.858801: Epoch   1 Batch  219/781   test_loss = 1.012\n",
      "2021-04-21T03:28:31.971678: Epoch   1 Batch  239/781   test_loss = 1.294\n",
      "2021-04-21T03:28:32.087858: Epoch   1 Batch  259/781   test_loss = 1.040\n",
      "2021-04-21T03:28:32.204142: Epoch   1 Batch  279/781   test_loss = 1.102\n",
      "2021-04-21T03:28:32.319099: Epoch   1 Batch  299/781   test_loss = 1.212\n",
      "2021-04-21T03:28:32.429063: Epoch   1 Batch  319/781   test_loss = 0.964\n",
      "2021-04-21T03:28:32.547315: Epoch   1 Batch  339/781   test_loss = 0.933\n",
      "2021-04-21T03:28:32.659481: Epoch   1 Batch  359/781   test_loss = 0.937\n",
      "2021-04-21T03:28:32.774930: Epoch   1 Batch  379/781   test_loss = 1.059\n",
      "2021-04-21T03:28:32.888466: Epoch   1 Batch  399/781   test_loss = 0.863\n",
      "2021-04-21T03:28:33.017148: Epoch   1 Batch  419/781   test_loss = 0.974\n",
      "2021-04-21T03:28:33.130601: Epoch   1 Batch  439/781   test_loss = 0.996\n",
      "2021-04-21T03:28:33.245868: Epoch   1 Batch  459/781   test_loss = 1.050\n",
      "2021-04-21T03:28:33.361692: Epoch   1 Batch  479/781   test_loss = 1.023\n",
      "2021-04-21T03:28:33.476424: Epoch   1 Batch  499/781   test_loss = 0.956\n",
      "2021-04-21T03:28:33.585792: Epoch   1 Batch  519/781   test_loss = 1.081\n",
      "2021-04-21T03:28:33.700170: Epoch   1 Batch  539/781   test_loss = 0.928\n",
      "2021-04-21T03:28:33.809589: Epoch   1 Batch  559/781   test_loss = 1.129\n",
      "2021-04-21T03:28:33.922449: Epoch   1 Batch  579/781   test_loss = 1.048\n",
      "2021-04-21T03:28:34.032685: Epoch   1 Batch  599/781   test_loss = 1.001\n",
      "2021-04-21T03:28:34.144633: Epoch   1 Batch  619/781   test_loss = 1.151\n",
      "2021-04-21T03:28:34.256352: Epoch   1 Batch  639/781   test_loss = 0.927\n",
      "2021-04-21T03:28:34.372799: Epoch   1 Batch  659/781   test_loss = 1.107\n",
      "2021-04-21T03:28:34.485011: Epoch   1 Batch  679/781   test_loss = 1.167\n",
      "2021-04-21T03:28:34.598024: Epoch   1 Batch  699/781   test_loss = 0.916\n",
      "2021-04-21T03:28:34.710858: Epoch   1 Batch  719/781   test_loss = 1.003\n",
      "2021-04-21T03:28:34.837275: Epoch   1 Batch  739/781   test_loss = 0.993\n",
      "2021-04-21T03:28:34.951942: Epoch   1 Batch  759/781   test_loss = 0.953\n",
      "2021-04-21T03:28:35.069054: Epoch   1 Batch  779/781   test_loss = 0.775\n",
      "2021-04-21T03:28:35.940724: Epoch   2 Batch   10/3125   train_loss = 0.986\n",
      "2021-04-21T03:28:36.342341: Epoch   2 Batch   30/3125   train_loss = 1.055\n",
      "2021-04-21T03:28:36.766511: Epoch   2 Batch   50/3125   train_loss = 1.022\n",
      "2021-04-21T03:28:37.167828: Epoch   2 Batch   70/3125   train_loss = 1.071\n",
      "2021-04-21T03:28:37.602642: Epoch   2 Batch   90/3125   train_loss = 1.025\n",
      "2021-04-21T03:28:38.003517: Epoch   2 Batch  110/3125   train_loss = 0.977\n",
      "2021-04-21T03:28:38.446883: Epoch   2 Batch  130/3125   train_loss = 0.989\n",
      "2021-04-21T03:28:38.837282: Epoch   2 Batch  150/3125   train_loss = 1.145\n",
      "2021-04-21T03:28:39.227846: Epoch   2 Batch  170/3125   train_loss = 1.061\n",
      "2021-04-21T03:28:39.647693: Epoch   2 Batch  190/3125   train_loss = 1.111\n",
      "2021-04-21T03:28:40.067545: Epoch   2 Batch  210/3125   train_loss = 0.923\n",
      "2021-04-21T03:28:40.483457: Epoch   2 Batch  230/3125   train_loss = 1.072\n",
      "2021-04-21T03:28:40.903336: Epoch   2 Batch  250/3125   train_loss = 0.918\n",
      "2021-04-21T03:28:41.317622: Epoch   2 Batch  270/3125   train_loss = 0.786\n",
      "2021-04-21T03:28:41.742512: Epoch   2 Batch  290/3125   train_loss = 1.046\n",
      "2021-04-21T03:28:42.143441: Epoch   2 Batch  310/3125   train_loss = 0.980\n",
      "2021-04-21T03:28:42.546414: Epoch   2 Batch  330/3125   train_loss = 1.034\n",
      "2021-04-21T03:28:42.953318: Epoch   2 Batch  350/3125   train_loss = 0.928\n",
      "2021-04-21T03:28:43.364200: Epoch   2 Batch  370/3125   train_loss = 1.177\n",
      "2021-04-21T03:28:43.763618: Epoch   2 Batch  390/3125   train_loss = 1.168\n",
      "2021-04-21T03:28:44.181503: Epoch   2 Batch  410/3125   train_loss = 0.949\n",
      "2021-04-21T03:28:44.574140: Epoch   2 Batch  430/3125   train_loss = 1.168\n",
      "2021-04-21T03:28:44.974417: Epoch   2 Batch  450/3125   train_loss = 0.938\n",
      "2021-04-21T03:28:45.373308: Epoch   2 Batch  470/3125   train_loss = 0.878\n",
      "2021-04-21T03:28:45.766514: Epoch   2 Batch  490/3125   train_loss = 1.003\n",
      "2021-04-21T03:28:46.164431: Epoch   2 Batch  510/3125   train_loss = 1.088\n",
      "2021-04-21T03:28:46.597101: Epoch   2 Batch  530/3125   train_loss = 0.963\n",
      "2021-04-21T03:28:47.008252: Epoch   2 Batch  550/3125   train_loss = 1.003\n",
      "2021-04-21T03:28:47.393903: Epoch   2 Batch  570/3125   train_loss = 1.096\n",
      "2021-04-21T03:28:47.850945: Epoch   2 Batch  590/3125   train_loss = 1.013\n",
      "2021-04-21T03:28:48.249098: Epoch   2 Batch  610/3125   train_loss = 0.919\n",
      "2021-04-21T03:28:48.693574: Epoch   2 Batch  630/3125   train_loss = 1.118\n",
      "2021-04-21T03:28:49.089067: Epoch   2 Batch  650/3125   train_loss = 1.020\n",
      "2021-04-21T03:28:49.490037: Epoch   2 Batch  670/3125   train_loss = 1.023\n",
      "2021-04-21T03:28:49.889942: Epoch   2 Batch  690/3125   train_loss = 0.988\n",
      "2021-04-21T03:28:50.294386: Epoch   2 Batch  710/3125   train_loss = 0.904\n",
      "2021-04-21T03:28:50.683523: Epoch   2 Batch  730/3125   train_loss = 0.866\n",
      "2021-04-21T03:28:51.099385: Epoch   2 Batch  750/3125   train_loss = 0.941\n",
      "2021-04-21T03:28:51.514273: Epoch   2 Batch  770/3125   train_loss = 0.919\n",
      "2021-04-21T03:28:51.908618: Epoch   2 Batch  790/3125   train_loss = 0.885\n",
      "2021-04-21T03:28:52.326156: Epoch   2 Batch  810/3125   train_loss = 0.854\n",
      "2021-04-21T03:28:52.734384: Epoch   2 Batch  830/3125   train_loss = 0.880\n",
      "2021-04-21T03:28:53.153906: Epoch   2 Batch  850/3125   train_loss = 1.032\n",
      "2021-04-21T03:28:53.563834: Epoch   2 Batch  870/3125   train_loss = 0.857\n",
      "2021-04-21T03:28:53.971268: Epoch   2 Batch  890/3125   train_loss = 0.902\n",
      "2021-04-21T03:28:54.392631: Epoch   2 Batch  910/3125   train_loss = 1.025\n",
      "2021-04-21T03:28:54.808519: Epoch   2 Batch  930/3125   train_loss = 1.079\n",
      "2021-04-21T03:28:55.251382: Epoch   2 Batch  950/3125   train_loss = 0.936\n",
      "2021-04-21T03:28:55.684769: Epoch   2 Batch  970/3125   train_loss = 1.042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-21T03:28:56.099543: Epoch   2 Batch  990/3125   train_loss = 0.888\n",
      "2021-04-21T03:28:56.495514: Epoch   2 Batch 1010/3125   train_loss = 1.138\n",
      "2021-04-21T03:28:56.927984: Epoch   2 Batch 1030/3125   train_loss = 0.926\n",
      "2021-04-21T03:28:57.338321: Epoch   2 Batch 1050/3125   train_loss = 0.931\n",
      "2021-04-21T03:28:57.810495: Epoch   2 Batch 1070/3125   train_loss = 1.009\n",
      "2021-04-21T03:28:58.233800: Epoch   2 Batch 1090/3125   train_loss = 1.114\n",
      "2021-04-21T03:28:58.665476: Epoch   2 Batch 1110/3125   train_loss = 1.122\n",
      "2021-04-21T03:28:59.066254: Epoch   2 Batch 1130/3125   train_loss = 0.974\n",
      "2021-04-21T03:28:59.469193: Epoch   2 Batch 1150/3125   train_loss = 0.968\n",
      "2021-04-21T03:28:59.874972: Epoch   2 Batch 1170/3125   train_loss = 0.958\n",
      "2021-04-21T03:29:00.291449: Epoch   2 Batch 1190/3125   train_loss = 1.015\n",
      "2021-04-21T03:29:00.678018: Epoch   2 Batch 1210/3125   train_loss = 0.890\n",
      "2021-04-21T03:29:01.079906: Epoch   2 Batch 1230/3125   train_loss = 0.838\n",
      "2021-04-21T03:29:01.504066: Epoch   2 Batch 1250/3125   train_loss = 0.998\n",
      "2021-04-21T03:29:01.920946: Epoch   2 Batch 1270/3125   train_loss = 1.024\n",
      "2021-04-21T03:29:02.338804: Epoch   2 Batch 1290/3125   train_loss = 0.932\n",
      "2021-04-21T03:29:02.754723: Epoch   2 Batch 1310/3125   train_loss = 1.079\n",
      "2021-04-21T03:29:03.204490: Epoch   2 Batch 1330/3125   train_loss = 1.128\n",
      "2021-04-21T03:29:03.599461: Epoch   2 Batch 1350/3125   train_loss = 0.897\n",
      "2021-04-21T03:29:04.019336: Epoch   2 Batch 1370/3125   train_loss = 0.827\n",
      "2021-04-21T03:29:04.430106: Epoch   2 Batch 1390/3125   train_loss = 1.024\n",
      "2021-04-21T03:29:04.867353: Epoch   2 Batch 1410/3125   train_loss = 0.940\n",
      "2021-04-21T03:29:05.256772: Epoch   2 Batch 1430/3125   train_loss = 1.021\n",
      "2021-04-21T03:29:05.671854: Epoch   2 Batch 1450/3125   train_loss = 0.962\n",
      "2021-04-21T03:29:06.056681: Epoch   2 Batch 1470/3125   train_loss = 1.044\n",
      "2021-04-21T03:29:06.434359: Epoch   2 Batch 1490/3125   train_loss = 1.092\n",
      "2021-04-21T03:29:06.834251: Epoch   2 Batch 1510/3125   train_loss = 0.948\n",
      "2021-04-21T03:29:07.227176: Epoch   2 Batch 1530/3125   train_loss = 1.099\n",
      "2021-04-21T03:29:07.608002: Epoch   2 Batch 1550/3125   train_loss = 0.891\n",
      "2021-04-21T03:29:08.025501: Epoch   2 Batch 1570/3125   train_loss = 0.916\n",
      "2021-04-21T03:29:08.444432: Epoch   2 Batch 1590/3125   train_loss = 1.033\n",
      "2021-04-21T03:29:08.831679: Epoch   2 Batch 1610/3125   train_loss = 0.941\n",
      "2021-04-21T03:29:09.204211: Epoch   2 Batch 1630/3125   train_loss = 1.029\n",
      "2021-04-21T03:29:09.585189: Epoch   2 Batch 1650/3125   train_loss = 0.831\n",
      "2021-04-21T03:29:09.981291: Epoch   2 Batch 1670/3125   train_loss = 0.848\n",
      "2021-04-21T03:29:10.365121: Epoch   2 Batch 1690/3125   train_loss = 0.996\n",
      "2021-04-21T03:29:10.756946: Epoch   2 Batch 1710/3125   train_loss = 0.913\n",
      "2021-04-21T03:29:11.166233: Epoch   2 Batch 1730/3125   train_loss = 1.034\n",
      "2021-04-21T03:29:11.561180: Epoch   2 Batch 1750/3125   train_loss = 0.885\n",
      "2021-04-21T03:29:11.945101: Epoch   2 Batch 1770/3125   train_loss = 1.190\n",
      "2021-04-21T03:29:12.335783: Epoch   2 Batch 1790/3125   train_loss = 1.038\n",
      "2021-04-21T03:29:12.731934: Epoch   2 Batch 1810/3125   train_loss = 1.022\n",
      "2021-04-21T03:29:13.151928: Epoch   2 Batch 1830/3125   train_loss = 1.053\n",
      "2021-04-21T03:29:13.568815: Epoch   2 Batch 1850/3125   train_loss = 0.969\n",
      "2021-04-21T03:29:14.028773: Epoch   2 Batch 1870/3125   train_loss = 0.979\n",
      "2021-04-21T03:29:14.537388: Epoch   2 Batch 1890/3125   train_loss = 0.839\n",
      "2021-04-21T03:29:14.954299: Epoch   2 Batch 1910/3125   train_loss = 0.950\n",
      "2021-04-21T03:29:15.340268: Epoch   2 Batch 1930/3125   train_loss = 1.012\n",
      "2021-04-21T03:29:15.737779: Epoch   2 Batch 1950/3125   train_loss = 0.839\n",
      "2021-04-21T03:29:16.152522: Epoch   2 Batch 1970/3125   train_loss = 1.020\n",
      "2021-04-21T03:29:16.562621: Epoch   2 Batch 1990/3125   train_loss = 0.847\n",
      "2021-04-21T03:29:17.074710: Epoch   2 Batch 2010/3125   train_loss = 0.837\n",
      "2021-04-21T03:29:17.460197: Epoch   2 Batch 2030/3125   train_loss = 1.017\n",
      "2021-04-21T03:29:17.874402: Epoch   2 Batch 2050/3125   train_loss = 0.984\n",
      "2021-04-21T03:29:18.291888: Epoch   2 Batch 2070/3125   train_loss = 0.911\n",
      "2021-04-21T03:29:18.757618: Epoch   2 Batch 2090/3125   train_loss = 0.879\n",
      "2021-04-21T03:29:19.160563: Epoch   2 Batch 2110/3125   train_loss = 1.026\n",
      "2021-04-21T03:29:19.582413: Epoch   2 Batch 2130/3125   train_loss = 0.908\n",
      "2021-04-21T03:29:20.004285: Epoch   2 Batch 2150/3125   train_loss = 1.015\n",
      "2021-04-21T03:29:20.399962: Epoch   2 Batch 2170/3125   train_loss = 0.856\n",
      "2021-04-21T03:29:21.012324: Epoch   2 Batch 2190/3125   train_loss = 0.984\n",
      "2021-04-21T03:29:21.608730: Epoch   2 Batch 2210/3125   train_loss = 0.963\n",
      "2021-04-21T03:29:22.187159: Epoch   2 Batch 2230/3125   train_loss = 0.886\n",
      "2021-04-21T03:29:22.670957: Epoch   2 Batch 2250/3125   train_loss = 1.044\n",
      "2021-04-21T03:29:23.077930: Epoch   2 Batch 2270/3125   train_loss = 0.869\n",
      "2021-04-21T03:29:23.474869: Epoch   2 Batch 2290/3125   train_loss = 0.890\n",
      "2021-04-21T03:29:23.908707: Epoch   2 Batch 2310/3125   train_loss = 0.934\n",
      "2021-04-21T03:29:24.325461: Epoch   2 Batch 2330/3125   train_loss = 1.067\n",
      "2021-04-21T03:29:24.715507: Epoch   2 Batch 2350/3125   train_loss = 1.039\n",
      "2021-04-21T03:29:25.170623: Epoch   2 Batch 2370/3125   train_loss = 0.936\n",
      "2021-04-21T03:29:25.629732: Epoch   2 Batch 2390/3125   train_loss = 1.040\n",
      "2021-04-21T03:29:26.051642: Epoch   2 Batch 2410/3125   train_loss = 1.088\n",
      "2021-04-21T03:29:26.424671: Epoch   2 Batch 2430/3125   train_loss = 0.934\n",
      "2021-04-21T03:29:26.913339: Epoch   2 Batch 2450/3125   train_loss = 0.941\n",
      "2021-04-21T03:29:27.306818: Epoch   2 Batch 2470/3125   train_loss = 1.014\n",
      "2021-04-21T03:29:27.697353: Epoch   2 Batch 2490/3125   train_loss = 1.067\n",
      "2021-04-21T03:29:28.154023: Epoch   2 Batch 2510/3125   train_loss = 1.088\n",
      "2021-04-21T03:29:28.624764: Epoch   2 Batch 2530/3125   train_loss = 0.781\n",
      "2021-04-21T03:29:29.008968: Epoch   2 Batch 2550/3125   train_loss = 1.012\n",
      "2021-04-21T03:29:29.409102: Epoch   2 Batch 2570/3125   train_loss = 1.032\n",
      "2021-04-21T03:29:29.789741: Epoch   2 Batch 2590/3125   train_loss = 1.003\n",
      "2021-04-21T03:29:30.178992: Epoch   2 Batch 2610/3125   train_loss = 1.078\n",
      "2021-04-21T03:29:30.588230: Epoch   2 Batch 2630/3125   train_loss = 0.668\n",
      "2021-04-21T03:29:30.998160: Epoch   2 Batch 2650/3125   train_loss = 0.936\n",
      "2021-04-21T03:29:31.406070: Epoch   2 Batch 2670/3125   train_loss = 1.040\n",
      "2021-04-21T03:29:31.787131: Epoch   2 Batch 2690/3125   train_loss = 0.967\n",
      "2021-04-21T03:29:32.173560: Epoch   2 Batch 2710/3125   train_loss = 0.860\n",
      "2021-04-21T03:29:32.575485: Epoch   2 Batch 2730/3125   train_loss = 1.148\n",
      "2021-04-21T03:29:32.965443: Epoch   2 Batch 2750/3125   train_loss = 1.002\n",
      "2021-04-21T03:29:33.354183: Epoch   2 Batch 2770/3125   train_loss = 0.969\n",
      "2021-04-21T03:29:33.734232: Epoch   2 Batch 2790/3125   train_loss = 0.904\n",
      "2021-04-21T03:29:34.124982: Epoch   2 Batch 2810/3125   train_loss = 0.986\n",
      "2021-04-21T03:29:34.499027: Epoch   2 Batch 2830/3125   train_loss = 0.826\n",
      "2021-04-21T03:29:34.886967: Epoch   2 Batch 2850/3125   train_loss = 0.983\n",
      "2021-04-21T03:29:35.260566: Epoch   2 Batch 2870/3125   train_loss = 0.795\n",
      "2021-04-21T03:29:35.638491: Epoch   2 Batch 2890/3125   train_loss = 0.856\n",
      "2021-04-21T03:29:36.012768: Epoch   2 Batch 2910/3125   train_loss = 0.984\n",
      "2021-04-21T03:29:36.393464: Epoch   2 Batch 2930/3125   train_loss = 0.797\n",
      "2021-04-21T03:29:36.800349: Epoch   2 Batch 2950/3125   train_loss = 1.089\n",
      "2021-04-21T03:29:37.175560: Epoch   2 Batch 2970/3125   train_loss = 0.917\n",
      "2021-04-21T03:29:37.563117: Epoch   2 Batch 2990/3125   train_loss = 0.915\n",
      "2021-04-21T03:29:37.950151: Epoch   2 Batch 3010/3125   train_loss = 0.951\n",
      "2021-04-21T03:29:38.363224: Epoch   2 Batch 3030/3125   train_loss = 0.964\n",
      "2021-04-21T03:29:38.782765: Epoch   2 Batch 3050/3125   train_loss = 0.939\n",
      "2021-04-21T03:29:39.175666: Epoch   2 Batch 3070/3125   train_loss = 0.831\n",
      "2021-04-21T03:29:39.565330: Epoch   2 Batch 3090/3125   train_loss = 0.836\n",
      "2021-04-21T03:29:39.959739: Epoch   2 Batch 3110/3125   train_loss = 0.845\n",
      "2021-04-21T03:29:40.375604: Epoch   2 Batch   18/781   test_loss = 0.768\n",
      "2021-04-21T03:29:40.484488: Epoch   2 Batch   38/781   test_loss = 0.890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-21T03:29:40.586509: Epoch   2 Batch   58/781   test_loss = 0.872\n",
      "2021-04-21T03:29:40.694412: Epoch   2 Batch   78/781   test_loss = 0.866\n",
      "2021-04-21T03:29:40.798477: Epoch   2 Batch   98/781   test_loss = 0.919\n",
      "2021-04-21T03:29:40.901508: Epoch   2 Batch  118/781   test_loss = 0.876\n",
      "2021-04-21T03:29:41.005608: Epoch   2 Batch  138/781   test_loss = 1.001\n",
      "2021-04-21T03:29:41.136178: Epoch   2 Batch  158/781   test_loss = 0.896\n",
      "2021-04-21T03:29:41.249860: Epoch   2 Batch  178/781   test_loss = 0.832\n",
      "2021-04-21T03:29:41.353633: Epoch   2 Batch  198/781   test_loss = 0.939\n",
      "2021-04-21T03:29:41.458609: Epoch   2 Batch  218/781   test_loss = 1.034\n",
      "2021-04-21T03:29:41.564728: Epoch   2 Batch  238/781   test_loss = 0.943\n",
      "2021-04-21T03:29:41.674435: Epoch   2 Batch  258/781   test_loss = 0.978\n",
      "2021-04-21T03:29:41.787114: Epoch   2 Batch  278/781   test_loss = 1.074\n",
      "2021-04-21T03:29:41.891351: Epoch   2 Batch  298/781   test_loss = 0.940\n",
      "2021-04-21T03:29:41.995980: Epoch   2 Batch  318/781   test_loss = 0.870\n",
      "2021-04-21T03:29:42.099297: Epoch   2 Batch  338/781   test_loss = 0.950\n",
      "2021-04-21T03:29:42.202689: Epoch   2 Batch  358/781   test_loss = 0.971\n",
      "2021-04-21T03:29:42.304345: Epoch   2 Batch  378/781   test_loss = 0.924\n",
      "2021-04-21T03:29:42.410795: Epoch   2 Batch  398/781   test_loss = 0.800\n",
      "2021-04-21T03:29:42.515681: Epoch   2 Batch  418/781   test_loss = 0.942\n",
      "2021-04-21T03:29:42.619100: Epoch   2 Batch  438/781   test_loss = 1.015\n",
      "2021-04-21T03:29:42.724218: Epoch   2 Batch  458/781   test_loss = 0.876\n",
      "2021-04-21T03:29:42.827490: Epoch   2 Batch  478/781   test_loss = 0.913\n",
      "2021-04-21T03:29:42.930707: Epoch   2 Batch  498/781   test_loss = 0.797\n",
      "2021-04-21T03:29:43.034448: Epoch   2 Batch  518/781   test_loss = 0.920\n",
      "2021-04-21T03:29:43.135179: Epoch   2 Batch  538/781   test_loss = 0.839\n",
      "2021-04-21T03:29:43.237905: Epoch   2 Batch  558/781   test_loss = 0.797\n",
      "2021-04-21T03:29:43.343022: Epoch   2 Batch  578/781   test_loss = 0.969\n",
      "2021-04-21T03:29:43.446094: Epoch   2 Batch  598/781   test_loss = 1.078\n",
      "2021-04-21T03:29:43.549501: Epoch   2 Batch  618/781   test_loss = 0.848\n",
      "2021-04-21T03:29:43.653569: Epoch   2 Batch  638/781   test_loss = 0.898\n",
      "2021-04-21T03:29:43.762216: Epoch   2 Batch  658/781   test_loss = 1.061\n",
      "2021-04-21T03:29:43.867828: Epoch   2 Batch  678/781   test_loss = 0.921\n",
      "2021-04-21T03:29:43.971626: Epoch   2 Batch  698/781   test_loss = 0.918\n",
      "2021-04-21T03:29:44.073184: Epoch   2 Batch  718/781   test_loss = 1.021\n",
      "2021-04-21T03:29:44.180056: Epoch   2 Batch  738/781   test_loss = 0.799\n",
      "2021-04-21T03:29:44.300679: Epoch   2 Batch  758/781   test_loss = 0.928\n",
      "2021-04-21T03:29:44.414378: Epoch   2 Batch  778/781   test_loss = 0.933\n",
      "2021-04-21T03:29:45.207102: Epoch   3 Batch    5/3125   train_loss = 0.917\n",
      "2021-04-21T03:29:45.609004: Epoch   3 Batch   25/3125   train_loss = 0.951\n",
      "2021-04-21T03:29:46.031652: Epoch   3 Batch   45/3125   train_loss = 0.833\n",
      "2021-04-21T03:29:46.440562: Epoch   3 Batch   65/3125   train_loss = 0.978\n",
      "2021-04-21T03:29:46.845338: Epoch   3 Batch   85/3125   train_loss = 0.870\n",
      "2021-04-21T03:29:47.252356: Epoch   3 Batch  105/3125   train_loss = 0.767\n",
      "2021-04-21T03:29:47.621262: Epoch   3 Batch  125/3125   train_loss = 0.870\n",
      "2021-04-21T03:29:48.005093: Epoch   3 Batch  145/3125   train_loss = 0.944\n",
      "2021-04-21T03:29:48.414000: Epoch   3 Batch  165/3125   train_loss = 0.930\n",
      "2021-04-21T03:29:48.838688: Epoch   3 Batch  185/3125   train_loss = 0.876\n",
      "2021-04-21T03:29:49.230127: Epoch   3 Batch  205/3125   train_loss = 0.799\n",
      "2021-04-21T03:29:49.619747: Epoch   3 Batch  225/3125   train_loss = 0.822\n",
      "2021-04-21T03:29:50.006707: Epoch   3 Batch  245/3125   train_loss = 1.114\n",
      "2021-04-21T03:29:50.400590: Epoch   3 Batch  265/3125   train_loss = 0.935\n",
      "2021-04-21T03:29:50.798549: Epoch   3 Batch  285/3125   train_loss = 0.912\n",
      "2021-04-21T03:29:51.239273: Epoch   3 Batch  305/3125   train_loss = 0.855\n",
      "2021-04-21T03:29:51.656289: Epoch   3 Batch  325/3125   train_loss = 0.941\n",
      "2021-04-21T03:29:52.095236: Epoch   3 Batch  345/3125   train_loss = 0.982\n",
      "2021-04-21T03:29:52.527009: Epoch   3 Batch  365/3125   train_loss = 0.913\n",
      "2021-04-21T03:29:52.937911: Epoch   3 Batch  385/3125   train_loss = 0.849\n",
      "2021-04-21T03:29:53.353800: Epoch   3 Batch  405/3125   train_loss = 0.847\n",
      "2021-04-21T03:29:53.763703: Epoch   3 Batch  425/3125   train_loss = 0.992\n",
      "2021-04-21T03:29:54.176303: Epoch   3 Batch  445/3125   train_loss = 0.971\n",
      "2021-04-21T03:29:54.605161: Epoch   3 Batch  465/3125   train_loss = 0.899\n",
      "2021-04-21T03:29:55.029029: Epoch   3 Batch  485/3125   train_loss = 1.033\n",
      "2021-04-21T03:29:55.443207: Epoch   3 Batch  505/3125   train_loss = 0.857\n",
      "2021-04-21T03:29:55.869068: Epoch   3 Batch  525/3125   train_loss = 1.013\n",
      "2021-04-21T03:29:56.302909: Epoch   3 Batch  545/3125   train_loss = 0.834\n",
      "2021-04-21T03:29:56.726796: Epoch   3 Batch  565/3125   train_loss = 1.097\n",
      "2021-04-21T03:29:57.126157: Epoch   3 Batch  585/3125   train_loss = 0.907\n",
      "2021-04-21T03:29:57.519102: Epoch   3 Batch  605/3125   train_loss = 0.892\n",
      "2021-04-21T03:29:57.907065: Epoch   3 Batch  625/3125   train_loss = 0.933\n",
      "2021-04-21T03:29:58.335919: Epoch   3 Batch  645/3125   train_loss = 0.987\n",
      "2021-04-21T03:29:58.758814: Epoch   3 Batch  665/3125   train_loss = 0.991\n",
      "2021-04-21T03:29:59.145007: Epoch   3 Batch  685/3125   train_loss = 0.904\n",
      "2021-04-21T03:29:59.529805: Epoch   3 Batch  705/3125   train_loss = 1.068\n",
      "2021-04-21T03:29:59.932297: Epoch   3 Batch  725/3125   train_loss = 0.896\n",
      "2021-04-21T03:30:00.343497: Epoch   3 Batch  745/3125   train_loss = 0.851\n",
      "2021-04-21T03:30:00.744430: Epoch   3 Batch  765/3125   train_loss = 0.863\n",
      "2021-04-21T03:30:01.141391: Epoch   3 Batch  785/3125   train_loss = 1.055\n",
      "2021-04-21T03:30:01.523034: Epoch   3 Batch  805/3125   train_loss = 0.836\n",
      "2021-04-21T03:30:01.913328: Epoch   3 Batch  825/3125   train_loss = 0.885\n",
      "2021-04-21T03:30:02.318492: Epoch   3 Batch  845/3125   train_loss = 0.934\n",
      "2021-04-21T03:30:02.726488: Epoch   3 Batch  865/3125   train_loss = 0.972\n",
      "2021-04-21T03:30:03.141859: Epoch   3 Batch  885/3125   train_loss = 0.919\n",
      "2021-04-21T03:30:03.539522: Epoch   3 Batch  905/3125   train_loss = 1.001\n",
      "2021-04-21T03:30:03.936499: Epoch   3 Batch  925/3125   train_loss = 0.979\n",
      "2021-04-21T03:30:04.328332: Epoch   3 Batch  945/3125   train_loss = 0.944\n",
      "2021-04-21T03:30:04.731411: Epoch   3 Batch  965/3125   train_loss = 0.780\n",
      "2021-04-21T03:30:05.145808: Epoch   3 Batch  985/3125   train_loss = 1.000\n",
      "2021-04-21T03:30:05.532745: Epoch   3 Batch 1005/3125   train_loss = 0.830\n",
      "2021-04-21T03:30:05.937688: Epoch   3 Batch 1025/3125   train_loss = 0.869\n",
      "2021-04-21T03:30:06.331872: Epoch   3 Batch 1045/3125   train_loss = 1.151\n",
      "2021-04-21T03:30:06.735791: Epoch   3 Batch 1065/3125   train_loss = 0.904\n",
      "2021-04-21T03:30:07.135331: Epoch   3 Batch 1085/3125   train_loss = 0.864\n",
      "2021-04-21T03:30:07.541188: Epoch   3 Batch 1105/3125   train_loss = 0.890\n",
      "2021-04-21T03:30:07.948155: Epoch   3 Batch 1125/3125   train_loss = 0.872\n",
      "2021-04-21T03:30:08.325092: Epoch   3 Batch 1145/3125   train_loss = 0.953\n",
      "2021-04-21T03:30:08.783997: Epoch   3 Batch 1165/3125   train_loss = 0.995\n",
      "2021-04-21T03:30:09.185903: Epoch   3 Batch 1185/3125   train_loss = 0.905\n",
      "2021-04-21T03:30:09.582908: Epoch   3 Batch 1205/3125   train_loss = 0.845\n",
      "2021-04-21T03:30:09.987890: Epoch   3 Batch 1225/3125   train_loss = 0.986\n",
      "2021-04-21T03:30:10.381903: Epoch   3 Batch 1245/3125   train_loss = 1.018\n",
      "2021-04-21T03:30:10.775827: Epoch   3 Batch 1265/3125   train_loss = 0.953\n",
      "2021-04-21T03:30:11.170051: Epoch   3 Batch 1285/3125   train_loss = 1.020\n",
      "2021-04-21T03:30:11.577910: Epoch   3 Batch 1305/3125   train_loss = 0.844\n",
      "2021-04-21T03:30:11.969353: Epoch   3 Batch 1325/3125   train_loss = 0.839\n",
      "2021-04-21T03:30:12.361325: Epoch   3 Batch 1345/3125   train_loss = 0.971\n",
      "2021-04-21T03:30:12.753322: Epoch   3 Batch 1365/3125   train_loss = 0.804\n",
      "2021-04-21T03:30:13.148931: Epoch   3 Batch 1385/3125   train_loss = 0.789\n",
      "2021-04-21T03:30:13.546867: Epoch   3 Batch 1405/3125   train_loss = 0.841\n",
      "2021-04-21T03:30:13.936965: Epoch   3 Batch 1425/3125   train_loss = 1.096\n",
      "2021-04-21T03:30:14.325201: Epoch   3 Batch 1445/3125   train_loss = 1.022\n",
      "2021-04-21T03:30:14.719317: Epoch   3 Batch 1465/3125   train_loss = 0.917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-21T03:30:15.125230: Epoch   3 Batch 1485/3125   train_loss = 0.995\n",
      "2021-04-21T03:30:15.540661: Epoch   3 Batch 1505/3125   train_loss = 0.785\n",
      "2021-04-21T03:30:15.929621: Epoch   3 Batch 1525/3125   train_loss = 0.803\n",
      "2021-04-21T03:30:16.329023: Epoch   3 Batch 1545/3125   train_loss = 0.935\n",
      "2021-04-21T03:30:16.752529: Epoch   3 Batch 1565/3125   train_loss = 0.905\n",
      "2021-04-21T03:30:17.157354: Epoch   3 Batch 1585/3125   train_loss = 0.844\n",
      "2021-04-21T03:30:17.560117: Epoch   3 Batch 1605/3125   train_loss = 0.910\n",
      "2021-04-21T03:30:17.933884: Epoch   3 Batch 1625/3125   train_loss = 1.003\n",
      "2021-04-21T03:30:18.328728: Epoch   3 Batch 1645/3125   train_loss = 0.942\n",
      "2021-04-21T03:30:18.802225: Epoch   3 Batch 1665/3125   train_loss = 0.942\n",
      "2021-04-21T03:30:19.192996: Epoch   3 Batch 1685/3125   train_loss = 0.973\n",
      "2021-04-21T03:30:19.600672: Epoch   3 Batch 1705/3125   train_loss = 0.907\n",
      "2021-04-21T03:30:19.997461: Epoch   3 Batch 1725/3125   train_loss = 0.834\n",
      "2021-04-21T03:30:20.397406: Epoch   3 Batch 1745/3125   train_loss = 0.796\n",
      "2021-04-21T03:30:20.797310: Epoch   3 Batch 1765/3125   train_loss = 0.832\n",
      "2021-04-21T03:30:21.197180: Epoch   3 Batch 1785/3125   train_loss = 1.055\n",
      "2021-04-21T03:30:21.608760: Epoch   3 Batch 1805/3125   train_loss = 0.980\n",
      "2021-04-21T03:30:22.010710: Epoch   3 Batch 1825/3125   train_loss = 1.039\n",
      "2021-04-21T03:30:22.399001: Epoch   3 Batch 1845/3125   train_loss = 0.987\n",
      "2021-04-21T03:30:22.791069: Epoch   3 Batch 1865/3125   train_loss = 0.815\n",
      "2021-04-21T03:30:23.179673: Epoch   3 Batch 1885/3125   train_loss = 0.980\n",
      "2021-04-21T03:30:23.573338: Epoch   3 Batch 1905/3125   train_loss = 0.819\n",
      "2021-04-21T03:30:23.980140: Epoch   3 Batch 1925/3125   train_loss = 0.876\n",
      "2021-04-21T03:30:24.372763: Epoch   3 Batch 1945/3125   train_loss = 0.896\n",
      "2021-04-21T03:30:24.765877: Epoch   3 Batch 1965/3125   train_loss = 0.851\n",
      "2021-04-21T03:30:25.155962: Epoch   3 Batch 1985/3125   train_loss = 0.806\n",
      "2021-04-21T03:30:25.565347: Epoch   3 Batch 2005/3125   train_loss = 0.926\n",
      "2021-04-21T03:30:25.962698: Epoch   3 Batch 2025/3125   train_loss = 0.943\n",
      "2021-04-21T03:30:26.362895: Epoch   3 Batch 2045/3125   train_loss = 0.825\n",
      "2021-04-21T03:30:26.786658: Epoch   3 Batch 2065/3125   train_loss = 0.786\n",
      "2021-04-21T03:30:27.184538: Epoch   3 Batch 2085/3125   train_loss = 1.045\n",
      "2021-04-21T03:30:27.586760: Epoch   3 Batch 2105/3125   train_loss = 0.880\n",
      "2021-04-21T03:30:27.993692: Epoch   3 Batch 2125/3125   train_loss = 0.970\n",
      "2021-04-21T03:30:28.394723: Epoch   3 Batch 2145/3125   train_loss = 1.019\n",
      "2021-04-21T03:30:28.866124: Epoch   3 Batch 2165/3125   train_loss = 0.861\n",
      "2021-04-21T03:30:29.257040: Epoch   3 Batch 2185/3125   train_loss = 0.918\n",
      "2021-04-21T03:30:29.657039: Epoch   3 Batch 2205/3125   train_loss = 0.916\n",
      "2021-04-21T03:30:30.060989: Epoch   3 Batch 2225/3125   train_loss = 0.877\n",
      "2021-04-21T03:30:30.487122: Epoch   3 Batch 2245/3125   train_loss = 0.815\n",
      "2021-04-21T03:30:30.891941: Epoch   3 Batch 2265/3125   train_loss = 0.986\n",
      "2021-04-21T03:30:31.283866: Epoch   3 Batch 2285/3125   train_loss = 1.082\n",
      "2021-04-21T03:30:31.682745: Epoch   3 Batch 2305/3125   train_loss = 0.811\n",
      "2021-04-21T03:30:32.088390: Epoch   3 Batch 2325/3125   train_loss = 0.864\n",
      "2021-04-21T03:30:32.478390: Epoch   3 Batch 2345/3125   train_loss = 0.864\n",
      "2021-04-21T03:30:32.871784: Epoch   3 Batch 2365/3125   train_loss = 0.733\n",
      "2021-04-21T03:30:33.264062: Epoch   3 Batch 2385/3125   train_loss = 0.956\n",
      "2021-04-21T03:30:33.652976: Epoch   3 Batch 2405/3125   train_loss = 0.930\n",
      "2021-04-21T03:30:34.053875: Epoch   3 Batch 2425/3125   train_loss = 0.863\n",
      "2021-04-21T03:30:34.449300: Epoch   3 Batch 2445/3125   train_loss = 0.948\n",
      "2021-04-21T03:30:34.848747: Epoch   3 Batch 2465/3125   train_loss = 0.748\n",
      "2021-04-21T03:30:35.247450: Epoch   3 Batch 2485/3125   train_loss = 0.884\n",
      "2021-04-21T03:30:35.645362: Epoch   3 Batch 2505/3125   train_loss = 0.888\n",
      "2021-04-21T03:30:36.053844: Epoch   3 Batch 2525/3125   train_loss = 0.846\n",
      "2021-04-21T03:30:36.449349: Epoch   3 Batch 2545/3125   train_loss = 0.989\n",
      "2021-04-21T03:30:36.875219: Epoch   3 Batch 2565/3125   train_loss = 0.829\n",
      "2021-04-21T03:30:37.276095: Epoch   3 Batch 2585/3125   train_loss = 0.789\n",
      "2021-04-21T03:30:37.668052: Epoch   3 Batch 2605/3125   train_loss = 0.849\n",
      "2021-04-21T03:30:38.058376: Epoch   3 Batch 2625/3125   train_loss = 0.985\n",
      "2021-04-21T03:30:38.472063: Epoch   3 Batch 2645/3125   train_loss = 0.898\n",
      "2021-04-21T03:30:38.934896: Epoch   3 Batch 2665/3125   train_loss = 0.961\n",
      "2021-04-21T03:30:39.330093: Epoch   3 Batch 2685/3125   train_loss = 0.897\n",
      "2021-04-21T03:30:39.721813: Epoch   3 Batch 2705/3125   train_loss = 0.786\n",
      "2021-04-21T03:30:40.131946: Epoch   3 Batch 2725/3125   train_loss = 0.968\n",
      "2021-04-21T03:30:40.519755: Epoch   3 Batch 2745/3125   train_loss = 0.917\n",
      "2021-04-21T03:30:40.908215: Epoch   3 Batch 2765/3125   train_loss = 0.852\n",
      "2021-04-21T03:30:41.302031: Epoch   3 Batch 2785/3125   train_loss = 0.937\n",
      "2021-04-21T03:30:41.688224: Epoch   3 Batch 2805/3125   train_loss = 0.892\n",
      "2021-04-21T03:30:42.095733: Epoch   3 Batch 2825/3125   train_loss = 0.895\n",
      "2021-04-21T03:30:42.494315: Epoch   3 Batch 2845/3125   train_loss = 0.938\n",
      "2021-04-21T03:30:42.902063: Epoch   3 Batch 2865/3125   train_loss = 0.854\n",
      "2021-04-21T03:30:43.303953: Epoch   3 Batch 2885/3125   train_loss = 0.940\n",
      "2021-04-21T03:30:43.713933: Epoch   3 Batch 2905/3125   train_loss = 0.926\n",
      "2021-04-21T03:30:44.120965: Epoch   3 Batch 2925/3125   train_loss = 0.880\n",
      "2021-04-21T03:30:44.511249: Epoch   3 Batch 2945/3125   train_loss = 0.979\n",
      "2021-04-21T03:30:44.950345: Epoch   3 Batch 2965/3125   train_loss = 0.977\n",
      "2021-04-21T03:30:45.392495: Epoch   3 Batch 2985/3125   train_loss = 0.839\n",
      "2021-04-21T03:30:45.787503: Epoch   3 Batch 3005/3125   train_loss = 0.807\n",
      "2021-04-21T03:30:46.174509: Epoch   3 Batch 3025/3125   train_loss = 0.935\n",
      "2021-04-21T03:30:46.589085: Epoch   3 Batch 3045/3125   train_loss = 0.939\n",
      "2021-04-21T03:30:46.997967: Epoch   3 Batch 3065/3125   train_loss = 0.808\n",
      "2021-04-21T03:30:47.399722: Epoch   3 Batch 3085/3125   train_loss = 0.862\n",
      "2021-04-21T03:30:47.803823: Epoch   3 Batch 3105/3125   train_loss = 0.922\n",
      "2021-04-21T03:30:48.270445: Epoch   3 Batch   17/781   test_loss = 0.901\n",
      "2021-04-21T03:30:48.391772: Epoch   3 Batch   37/781   test_loss = 0.884\n",
      "2021-04-21T03:30:48.508433: Epoch   3 Batch   57/781   test_loss = 0.938\n",
      "2021-04-21T03:30:48.622660: Epoch   3 Batch   77/781   test_loss = 0.823\n",
      "2021-04-21T03:30:48.814610: Epoch   3 Batch   97/781   test_loss = 0.760\n",
      "2021-04-21T03:30:48.945370: Epoch   3 Batch  117/781   test_loss = 0.985\n",
      "2021-04-21T03:30:49.068172: Epoch   3 Batch  137/781   test_loss = 0.904\n",
      "2021-04-21T03:30:49.198797: Epoch   3 Batch  157/781   test_loss = 0.926\n",
      "2021-04-21T03:30:49.312285: Epoch   3 Batch  177/781   test_loss = 0.882\n",
      "2021-04-21T03:30:49.422988: Epoch   3 Batch  197/781   test_loss = 0.905\n",
      "2021-04-21T03:30:49.539261: Epoch   3 Batch  217/781   test_loss = 0.737\n",
      "2021-04-21T03:30:49.653142: Epoch   3 Batch  237/781   test_loss = 0.789\n",
      "2021-04-21T03:30:49.771793: Epoch   3 Batch  257/781   test_loss = 1.000\n",
      "2021-04-21T03:30:49.886587: Epoch   3 Batch  277/781   test_loss = 1.000\n",
      "2021-04-21T03:30:50.003262: Epoch   3 Batch  297/781   test_loss = 0.977\n",
      "2021-04-21T03:30:50.115455: Epoch   3 Batch  317/781   test_loss = 0.990\n",
      "2021-04-21T03:30:50.235021: Epoch   3 Batch  337/781   test_loss = 0.936\n",
      "2021-04-21T03:30:50.348190: Epoch   3 Batch  357/781   test_loss = 0.940\n",
      "2021-04-21T03:30:50.467592: Epoch   3 Batch  377/781   test_loss = 0.957\n",
      "2021-04-21T03:30:50.582644: Epoch   3 Batch  397/781   test_loss = 0.951\n",
      "2021-04-21T03:30:50.700832: Epoch   3 Batch  417/781   test_loss = 0.814\n",
      "2021-04-21T03:30:50.815263: Epoch   3 Batch  437/781   test_loss = 0.805\n",
      "2021-04-21T03:30:50.929767: Epoch   3 Batch  457/781   test_loss = 0.737\n",
      "2021-04-21T03:30:51.042307: Epoch   3 Batch  477/781   test_loss = 0.953\n",
      "2021-04-21T03:30:51.159373: Epoch   3 Batch  497/781   test_loss = 0.780\n",
      "2021-04-21T03:30:51.272317: Epoch   3 Batch  517/781   test_loss = 0.852\n",
      "2021-04-21T03:30:51.385837: Epoch   3 Batch  537/781   test_loss = 0.904\n",
      "2021-04-21T03:30:51.499063: Epoch   3 Batch  557/781   test_loss = 0.989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-21T03:30:51.611504: Epoch   3 Batch  577/781   test_loss = 0.926\n",
      "2021-04-21T03:30:51.728287: Epoch   3 Batch  597/781   test_loss = 0.862\n",
      "2021-04-21T03:30:51.860291: Epoch   3 Batch  617/781   test_loss = 0.920\n",
      "2021-04-21T03:30:51.973167: Epoch   3 Batch  637/781   test_loss = 0.837\n",
      "2021-04-21T03:30:52.090937: Epoch   3 Batch  657/781   test_loss = 1.012\n",
      "2021-04-21T03:30:52.203292: Epoch   3 Batch  677/781   test_loss = 0.983\n",
      "2021-04-21T03:30:52.317172: Epoch   3 Batch  697/781   test_loss = 0.949\n",
      "2021-04-21T03:30:52.429871: Epoch   3 Batch  717/781   test_loss = 0.819\n",
      "2021-04-21T03:30:52.551545: Epoch   3 Batch  737/781   test_loss = 0.774\n",
      "2021-04-21T03:30:52.671411: Epoch   3 Batch  757/781   test_loss = 1.042\n",
      "2021-04-21T03:30:52.786454: Epoch   3 Batch  777/781   test_loss = 0.960\n",
      "2021-04-21T03:30:53.479626: Epoch   4 Batch    0/3125   train_loss = 0.963\n",
      "2021-04-21T03:30:53.871862: Epoch   4 Batch   20/3125   train_loss = 0.874\n",
      "2021-04-21T03:30:54.256776: Epoch   4 Batch   40/3125   train_loss = 0.920\n",
      "2021-04-21T03:30:54.659699: Epoch   4 Batch   60/3125   train_loss = 0.784\n",
      "2021-04-21T03:30:55.070542: Epoch   4 Batch   80/3125   train_loss = 0.858\n",
      "2021-04-21T03:30:55.461522: Epoch   4 Batch  100/3125   train_loss = 0.881\n",
      "2021-04-21T03:30:55.844521: Epoch   4 Batch  120/3125   train_loss = 0.943\n",
      "2021-04-21T03:30:56.238535: Epoch   4 Batch  140/3125   train_loss = 0.943\n",
      "2021-04-21T03:30:56.639406: Epoch   4 Batch  160/3125   train_loss = 0.780\n",
      "2021-04-21T03:30:57.039297: Epoch   4 Batch  180/3125   train_loss = 0.909\n",
      "2021-04-21T03:30:57.434238: Epoch   4 Batch  200/3125   train_loss = 1.141\n",
      "2021-04-21T03:30:57.830331: Epoch   4 Batch  220/3125   train_loss = 0.921\n",
      "2021-04-21T03:30:58.219522: Epoch   4 Batch  240/3125   train_loss = 1.003\n",
      "2021-04-21T03:30:58.618829: Epoch   4 Batch  260/3125   train_loss = 0.984\n",
      "2021-04-21T03:30:59.092982: Epoch   4 Batch  280/3125   train_loss = 0.917\n",
      "2021-04-21T03:30:59.488776: Epoch   4 Batch  300/3125   train_loss = 1.055\n",
      "2021-04-21T03:30:59.880760: Epoch   4 Batch  320/3125   train_loss = 0.997\n",
      "2021-04-21T03:31:00.296739: Epoch   4 Batch  340/3125   train_loss = 0.776\n",
      "2021-04-21T03:31:00.692702: Epoch   4 Batch  360/3125   train_loss = 0.836\n",
      "2021-04-21T03:31:01.088273: Epoch   4 Batch  380/3125   train_loss = 0.870\n",
      "2021-04-21T03:31:01.487233: Epoch   4 Batch  400/3125   train_loss = 0.835\n",
      "2021-04-21T03:31:01.879184: Epoch   4 Batch  420/3125   train_loss = 0.809\n",
      "2021-04-21T03:31:02.283302: Epoch   4 Batch  440/3125   train_loss = 0.875\n",
      "2021-04-21T03:31:02.666308: Epoch   4 Batch  460/3125   train_loss = 0.878\n",
      "2021-04-21T03:31:03.065237: Epoch   4 Batch  480/3125   train_loss = 0.998\n",
      "2021-04-21T03:31:03.464549: Epoch   4 Batch  500/3125   train_loss = 0.660\n",
      "2021-04-21T03:31:03.861554: Epoch   4 Batch  520/3125   train_loss = 0.928\n",
      "2021-04-21T03:31:04.241844: Epoch   4 Batch  540/3125   train_loss = 0.872\n",
      "2021-04-21T03:31:04.636647: Epoch   4 Batch  560/3125   train_loss = 1.033\n",
      "2021-04-21T03:31:05.052719: Epoch   4 Batch  580/3125   train_loss = 0.992\n",
      "2021-04-21T03:31:05.443673: Epoch   4 Batch  600/3125   train_loss = 0.875\n",
      "2021-04-21T03:31:05.851858: Epoch   4 Batch  620/3125   train_loss = 0.954\n",
      "2021-04-21T03:31:06.243373: Epoch   4 Batch  640/3125   train_loss = 0.919\n",
      "2021-04-21T03:31:06.661039: Epoch   4 Batch  660/3125   train_loss = 0.925\n",
      "2021-04-21T03:31:07.078445: Epoch   4 Batch  680/3125   train_loss = 0.949\n",
      "2021-04-21T03:31:07.475870: Epoch   4 Batch  700/3125   train_loss = 0.885\n",
      "2021-04-21T03:31:07.884805: Epoch   4 Batch  720/3125   train_loss = 0.788\n",
      "2021-04-21T03:31:08.284815: Epoch   4 Batch  740/3125   train_loss = 0.885\n",
      "2021-04-21T03:31:08.677699: Epoch   4 Batch  760/3125   train_loss = 0.824\n",
      "2021-04-21T03:31:09.173117: Epoch   4 Batch  780/3125   train_loss = 0.959\n",
      "2021-04-21T03:31:09.563190: Epoch   4 Batch  800/3125   train_loss = 0.786\n",
      "2021-04-21T03:31:10.003172: Epoch   4 Batch  820/3125   train_loss = 0.878\n",
      "2021-04-21T03:31:10.390745: Epoch   4 Batch  840/3125   train_loss = 0.839\n",
      "2021-04-21T03:31:10.792005: Epoch   4 Batch  860/3125   train_loss = 0.879\n",
      "2021-04-21T03:31:11.190938: Epoch   4 Batch  880/3125   train_loss = 0.794\n",
      "2021-04-21T03:31:11.582830: Epoch   4 Batch  900/3125   train_loss = 0.881\n",
      "2021-04-21T03:31:11.983385: Epoch   4 Batch  920/3125   train_loss = 0.921\n",
      "2021-04-21T03:31:12.406736: Epoch   4 Batch  940/3125   train_loss = 0.908\n",
      "2021-04-21T03:31:12.805588: Epoch   4 Batch  960/3125   train_loss = 0.927\n",
      "2021-04-21T03:31:13.203008: Epoch   4 Batch  980/3125   train_loss = 0.991\n",
      "2021-04-21T03:31:13.596950: Epoch   4 Batch 1000/3125   train_loss = 0.990\n",
      "2021-04-21T03:31:13.992239: Epoch   4 Batch 1020/3125   train_loss = 0.870\n",
      "2021-04-21T03:31:14.395557: Epoch   4 Batch 1040/3125   train_loss = 0.810\n",
      "2021-04-21T03:31:14.794202: Epoch   4 Batch 1060/3125   train_loss = 0.968\n",
      "2021-04-21T03:31:15.207840: Epoch   4 Batch 1080/3125   train_loss = 0.921\n",
      "2021-04-21T03:31:15.609969: Epoch   4 Batch 1100/3125   train_loss = 0.873\n",
      "2021-04-21T03:31:16.000047: Epoch   4 Batch 1120/3125   train_loss = 0.859\n",
      "2021-04-21T03:31:16.407956: Epoch   4 Batch 1140/3125   train_loss = 0.857\n",
      "2021-04-21T03:31:16.841020: Epoch   4 Batch 1160/3125   train_loss = 0.848\n",
      "2021-04-21T03:31:17.250553: Epoch   4 Batch 1180/3125   train_loss = 0.861\n",
      "2021-04-21T03:31:17.638515: Epoch   4 Batch 1200/3125   train_loss = 0.985\n",
      "2021-04-21T03:31:18.033004: Epoch   4 Batch 1220/3125   train_loss = 0.926\n",
      "2021-04-21T03:31:18.429517: Epoch   4 Batch 1240/3125   train_loss = 0.785\n",
      "2021-04-21T03:31:18.846341: Epoch   4 Batch 1260/3125   train_loss = 0.933\n",
      "2021-04-21T03:31:19.291177: Epoch   4 Batch 1280/3125   train_loss = 0.880\n",
      "2021-04-21T03:31:19.697757: Epoch   4 Batch 1300/3125   train_loss = 0.856\n",
      "2021-04-21T03:31:20.090038: Epoch   4 Batch 1320/3125   train_loss = 0.855\n",
      "2021-04-21T03:31:20.490722: Epoch   4 Batch 1340/3125   train_loss = 0.747\n",
      "2021-04-21T03:31:20.900932: Epoch   4 Batch 1360/3125   train_loss = 0.835\n",
      "2021-04-21T03:31:21.303714: Epoch   4 Batch 1380/3125   train_loss = 0.840\n",
      "2021-04-21T03:31:21.710903: Epoch   4 Batch 1400/3125   train_loss = 0.929\n",
      "2021-04-21T03:31:22.103727: Epoch   4 Batch 1420/3125   train_loss = 0.878\n",
      "2021-04-21T03:31:22.499588: Epoch   4 Batch 1440/3125   train_loss = 0.756\n",
      "2021-04-21T03:31:22.899760: Epoch   4 Batch 1460/3125   train_loss = 0.892\n",
      "2021-04-21T03:31:23.290133: Epoch   4 Batch 1480/3125   train_loss = 0.891\n",
      "2021-04-21T03:31:23.693798: Epoch   4 Batch 1500/3125   train_loss = 0.896\n",
      "2021-04-21T03:31:24.098786: Epoch   4 Batch 1520/3125   train_loss = 0.812\n",
      "2021-04-21T03:31:24.493704: Epoch   4 Batch 1540/3125   train_loss = 0.943\n",
      "2021-04-21T03:31:24.895223: Epoch   4 Batch 1560/3125   train_loss = 0.776\n",
      "2021-04-21T03:31:25.295138: Epoch   4 Batch 1580/3125   train_loss = 0.990\n",
      "2021-04-21T03:31:25.687700: Epoch   4 Batch 1600/3125   train_loss = 0.830\n",
      "2021-04-21T03:31:26.083468: Epoch   4 Batch 1620/3125   train_loss = 0.849\n",
      "2021-04-21T03:31:26.489458: Epoch   4 Batch 1640/3125   train_loss = 0.992\n",
      "2021-04-21T03:31:26.903351: Epoch   4 Batch 1660/3125   train_loss = 0.998\n",
      "2021-04-21T03:31:27.305221: Epoch   4 Batch 1680/3125   train_loss = 0.883\n",
      "2021-04-21T03:31:27.715125: Epoch   4 Batch 1700/3125   train_loss = 0.815\n",
      "2021-04-21T03:31:28.118610: Epoch   4 Batch 1720/3125   train_loss = 0.881\n",
      "2021-04-21T03:31:28.526177: Epoch   4 Batch 1740/3125   train_loss = 0.917\n",
      "2021-04-21T03:31:28.964478: Epoch   4 Batch 1760/3125   train_loss = 0.874\n",
      "2021-04-21T03:31:29.401226: Epoch   4 Batch 1780/3125   train_loss = 0.875\n",
      "2021-04-21T03:31:29.800327: Epoch   4 Batch 1800/3125   train_loss = 0.839\n",
      "2021-04-21T03:31:30.207597: Epoch   4 Batch 1820/3125   train_loss = 0.846\n",
      "2021-04-21T03:31:30.612132: Epoch   4 Batch 1840/3125   train_loss = 0.975\n",
      "2021-04-21T03:31:31.013778: Epoch   4 Batch 1860/3125   train_loss = 0.928\n",
      "2021-04-21T03:31:31.417939: Epoch   4 Batch 1880/3125   train_loss = 0.899\n",
      "2021-04-21T03:31:31.811427: Epoch   4 Batch 1900/3125   train_loss = 0.778\n",
      "2021-04-21T03:31:32.210911: Epoch   4 Batch 1920/3125   train_loss = 0.881\n",
      "2021-04-21T03:31:32.616559: Epoch   4 Batch 1940/3125   train_loss = 0.794\n",
      "2021-04-21T03:31:33.017548: Epoch   4 Batch 1960/3125   train_loss = 0.797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-21T03:31:33.417992: Epoch   4 Batch 1980/3125   train_loss = 0.888\n",
      "2021-04-21T03:31:33.816092: Epoch   4 Batch 2000/3125   train_loss = 1.058\n",
      "2021-04-21T03:31:34.213414: Epoch   4 Batch 2020/3125   train_loss = 0.956\n",
      "2021-04-21T03:31:34.615340: Epoch   4 Batch 2040/3125   train_loss = 0.761\n",
      "2021-04-21T03:31:35.030808: Epoch   4 Batch 2060/3125   train_loss = 0.854\n",
      "2021-04-21T03:31:35.436521: Epoch   4 Batch 2080/3125   train_loss = 1.002\n",
      "2021-04-21T03:31:35.826524: Epoch   4 Batch 2100/3125   train_loss = 0.839\n",
      "2021-04-21T03:31:36.222914: Epoch   4 Batch 2120/3125   train_loss = 0.784\n",
      "2021-04-21T03:31:36.649873: Epoch   4 Batch 2140/3125   train_loss = 0.907\n",
      "2021-04-21T03:31:37.050864: Epoch   4 Batch 2160/3125   train_loss = 0.828\n",
      "2021-04-21T03:31:37.453231: Epoch   4 Batch 2180/3125   train_loss = 0.933\n",
      "2021-04-21T03:31:37.858289: Epoch   4 Batch 2200/3125   train_loss = 0.852\n",
      "2021-04-21T03:31:38.257385: Epoch   4 Batch 2220/3125   train_loss = 0.862\n",
      "2021-04-21T03:31:38.668543: Epoch   4 Batch 2240/3125   train_loss = 0.890\n",
      "2021-04-21T03:31:39.097371: Epoch   4 Batch 2260/3125   train_loss = 0.919\n",
      "2021-04-21T03:31:39.547564: Epoch   4 Batch 2280/3125   train_loss = 0.870\n",
      "2021-04-21T03:31:39.956163: Epoch   4 Batch 2300/3125   train_loss = 0.832\n",
      "2021-04-21T03:31:40.363033: Epoch   4 Batch 2320/3125   train_loss = 0.936\n",
      "2021-04-21T03:31:40.763429: Epoch   4 Batch 2340/3125   train_loss = 0.856\n",
      "2021-04-21T03:31:41.154412: Epoch   4 Batch 2360/3125   train_loss = 0.880\n",
      "2021-04-21T03:31:41.576304: Epoch   4 Batch 2380/3125   train_loss = 0.822\n",
      "2021-04-21T03:31:41.982762: Epoch   4 Batch 2400/3125   train_loss = 0.976\n",
      "2021-04-21T03:31:42.383340: Epoch   4 Batch 2420/3125   train_loss = 0.825\n",
      "2021-04-21T03:31:42.790451: Epoch   4 Batch 2440/3125   train_loss = 0.858\n",
      "2021-04-21T03:31:43.194973: Epoch   4 Batch 2460/3125   train_loss = 0.869\n",
      "2021-04-21T03:31:43.602116: Epoch   4 Batch 2480/3125   train_loss = 1.000\n",
      "2021-04-21T03:31:44.004049: Epoch   4 Batch 2500/3125   train_loss = 0.792\n",
      "2021-04-21T03:31:44.403569: Epoch   4 Batch 2520/3125   train_loss = 0.937\n",
      "2021-04-21T03:31:44.809410: Epoch   4 Batch 2540/3125   train_loss = 0.816\n",
      "2021-04-21T03:31:45.201771: Epoch   4 Batch 2560/3125   train_loss = 0.691\n",
      "2021-04-21T03:31:45.598745: Epoch   4 Batch 2580/3125   train_loss = 0.852\n",
      "2021-04-21T03:31:46.009190: Epoch   4 Batch 2600/3125   train_loss = 0.882\n",
      "2021-04-21T03:31:46.410015: Epoch   4 Batch 2620/3125   train_loss = 0.777\n",
      "2021-04-21T03:31:46.826877: Epoch   4 Batch 2640/3125   train_loss = 0.820\n",
      "2021-04-21T03:31:47.226813: Epoch   4 Batch 2660/3125   train_loss = 0.982\n",
      "2021-04-21T03:31:47.617873: Epoch   4 Batch 2680/3125   train_loss = 0.797\n",
      "2021-04-21T03:31:48.023790: Epoch   4 Batch 2700/3125   train_loss = 0.848\n",
      "2021-04-21T03:31:48.417627: Epoch   4 Batch 2720/3125   train_loss = 0.770\n",
      "2021-04-21T03:31:48.818205: Epoch   4 Batch 2740/3125   train_loss = 0.898\n",
      "2021-04-21T03:31:49.258574: Epoch   4 Batch 2760/3125   train_loss = 0.774\n",
      "2021-04-21T03:31:49.695170: Epoch   4 Batch 2780/3125   train_loss = 0.824\n",
      "2021-04-21T03:31:50.098187: Epoch   4 Batch 2800/3125   train_loss = 1.107\n",
      "2021-04-21T03:31:50.504511: Epoch   4 Batch 2820/3125   train_loss = 1.080\n",
      "2021-04-21T03:31:50.893141: Epoch   4 Batch 2840/3125   train_loss = 0.832\n",
      "2021-04-21T03:31:51.292037: Epoch   4 Batch 2860/3125   train_loss = 0.797\n",
      "2021-04-21T03:31:51.686907: Epoch   4 Batch 2880/3125   train_loss = 0.832\n",
      "2021-04-21T03:31:52.082641: Epoch   4 Batch 2900/3125   train_loss = 0.892\n",
      "2021-04-21T03:31:52.479922: Epoch   4 Batch 2920/3125   train_loss = 0.894\n",
      "2021-04-21T03:31:52.874885: Epoch   4 Batch 2940/3125   train_loss = 0.949\n",
      "2021-04-21T03:31:53.270653: Epoch   4 Batch 2960/3125   train_loss = 0.875\n",
      "2021-04-21T03:31:53.666405: Epoch   4 Batch 2980/3125   train_loss = 0.829\n",
      "2021-04-21T03:31:54.055913: Epoch   4 Batch 3000/3125   train_loss = 0.944\n",
      "2021-04-21T03:31:54.459851: Epoch   4 Batch 3020/3125   train_loss = 1.022\n",
      "2021-04-21T03:31:54.872500: Epoch   4 Batch 3040/3125   train_loss = 0.873\n",
      "2021-04-21T03:31:55.270895: Epoch   4 Batch 3060/3125   train_loss = 0.762\n",
      "2021-04-21T03:31:55.674816: Epoch   4 Batch 3080/3125   train_loss = 0.989\n",
      "2021-04-21T03:31:56.082007: Epoch   4 Batch 3100/3125   train_loss = 1.019\n",
      "2021-04-21T03:31:56.471360: Epoch   4 Batch 3120/3125   train_loss = 0.843\n",
      "2021-04-21T03:31:56.667854: Epoch   4 Batch   16/781   test_loss = 0.808\n",
      "2021-04-21T03:31:56.789778: Epoch   4 Batch   36/781   test_loss = 0.914\n",
      "2021-04-21T03:31:56.909849: Epoch   4 Batch   56/781   test_loss = 0.957\n",
      "2021-04-21T03:31:57.035267: Epoch   4 Batch   76/781   test_loss = 0.974\n",
      "2021-04-21T03:31:57.153449: Epoch   4 Batch   96/781   test_loss = 0.993\n",
      "2021-04-21T03:31:57.274947: Epoch   4 Batch  116/781   test_loss = 0.831\n",
      "2021-04-21T03:31:57.388935: Epoch   4 Batch  136/781   test_loss = 0.826\n",
      "2021-04-21T03:31:57.503371: Epoch   4 Batch  156/781   test_loss = 0.865\n",
      "2021-04-21T03:31:57.618167: Epoch   4 Batch  176/781   test_loss = 0.875\n",
      "2021-04-21T03:31:57.730078: Epoch   4 Batch  196/781   test_loss = 0.805\n",
      "2021-04-21T03:31:57.843828: Epoch   4 Batch  216/781   test_loss = 0.930\n",
      "2021-04-21T03:31:57.956553: Epoch   4 Batch  236/781   test_loss = 0.842\n",
      "2021-04-21T03:31:58.069624: Epoch   4 Batch  256/781   test_loss = 0.821\n",
      "2021-04-21T03:31:58.195368: Epoch   4 Batch  276/781   test_loss = 1.112\n",
      "2021-04-21T03:31:58.307487: Epoch   4 Batch  296/781   test_loss = 0.850\n",
      "2021-04-21T03:31:58.424288: Epoch   4 Batch  316/781   test_loss = 0.865\n",
      "2021-04-21T03:31:58.534964: Epoch   4 Batch  336/781   test_loss = 0.755\n",
      "2021-04-21T03:31:58.650782: Epoch   4 Batch  356/781   test_loss = 0.833\n",
      "2021-04-21T03:31:58.762459: Epoch   4 Batch  376/781   test_loss = 0.903\n",
      "2021-04-21T03:31:58.882100: Epoch   4 Batch  396/781   test_loss = 0.862\n",
      "2021-04-21T03:31:59.035023: Epoch   4 Batch  416/781   test_loss = 0.936\n",
      "2021-04-21T03:31:59.154389: Epoch   4 Batch  436/781   test_loss = 0.907\n",
      "2021-04-21T03:31:59.269282: Epoch   4 Batch  456/781   test_loss = 0.739\n",
      "2021-04-21T03:31:59.382672: Epoch   4 Batch  476/781   test_loss = 0.988\n",
      "2021-04-21T03:31:59.550558: Epoch   4 Batch  496/781   test_loss = 1.009\n",
      "2021-04-21T03:31:59.668575: Epoch   4 Batch  516/781   test_loss = 0.745\n",
      "2021-04-21T03:31:59.785517: Epoch   4 Batch  536/781   test_loss = 0.925\n",
      "2021-04-21T03:31:59.897024: Epoch   4 Batch  556/781   test_loss = 0.836\n",
      "2021-04-21T03:32:00.031567: Epoch   4 Batch  576/781   test_loss = 0.957\n",
      "2021-04-21T03:32:00.147104: Epoch   4 Batch  596/781   test_loss = 1.004\n",
      "2021-04-21T03:32:00.265434: Epoch   4 Batch  616/781   test_loss = 0.923\n",
      "2021-04-21T03:32:00.382849: Epoch   4 Batch  636/781   test_loss = 0.840\n",
      "2021-04-21T03:32:00.496091: Epoch   4 Batch  656/781   test_loss = 0.868\n",
      "2021-04-21T03:32:00.612886: Epoch   4 Batch  676/781   test_loss = 1.066\n",
      "2021-04-21T03:32:00.729939: Epoch   4 Batch  696/781   test_loss = 0.881\n",
      "2021-04-21T03:32:00.842016: Epoch   4 Batch  716/781   test_loss = 0.903\n",
      "2021-04-21T03:32:00.962698: Epoch   4 Batch  736/781   test_loss = 1.024\n",
      "2021-04-21T03:32:01.076538: Epoch   4 Batch  756/781   test_loss = 0.860\n",
      "2021-04-21T03:32:01.190294: Epoch   4 Batch  776/781   test_loss = 0.769\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from models_function import get_batches\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "losses = {'train':[], 'test':[]}\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    \n",
    "    # Collect data for tensorBoard\n",
    "    # Keep track of gradient values and sparsity\n",
    "    grad_summaries = []\n",
    "    for g, v in gradients:\n",
    "        if g is not None:\n",
    "            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name.replace(':', '_')), g)\n",
    "            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name.replace(':', '_')), tf.nn.zero_fraction(g))\n",
    "            grad_summaries.append(grad_hist_summary)\n",
    "            grad_summaries.append(sparsity_summary)\n",
    "    grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "        \n",
    "    # Output directory for models and summaries\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "     \n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "    # Inference summaries\n",
    "    inference_summary_op = tf.summary.merge([loss_summary])\n",
    "    inference_summary_dir = os.path.join(out_dir, \"summaries\", \"inference\")\n",
    "    inference_summary_writer = tf.summary.FileWriter(inference_summary_dir, sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    for epoch_i in range(num_epochs):\n",
    "        \n",
    "        # Divide the data set into training set and test set, random seed is not fixed\n",
    "        train_X,test_X, train_y, test_y = train_test_split(features,  \n",
    "                                                           targets_values,  \n",
    "                                                           test_size = 0.2,  \n",
    "                                                           random_state = 0)  \n",
    "        \n",
    "        train_batches = get_batches(train_X, train_y, batch_size)\n",
    "        test_batches = get_batches(test_X, test_y, batch_size)\n",
    "    \n",
    "        # Training iterations, save training loss\n",
    "        for batch_i in range(len(train_X) // batch_size):\n",
    "            x, y = next(train_batches)\n",
    "\n",
    "            categories = np.zeros([batch_size, 18])\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6,1)[i]\n",
    "\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5,1)[i]\n",
    "\n",
    "            feed = {\n",
    "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
    "                movie_categories: categories,  #x.take(6,1)\n",
    "                movie_titles: titles,  #x.take(5,1)\n",
    "                targets: np.reshape(y, [batch_size, 1]),\n",
    "                dropout_keep_prob: dropout_keep, #dropout_keep\n",
    "                lr: learning_rate}\n",
    "\n",
    "            step, train_loss, summaries, _ = sess.run([global_step, loss, train_summary_op, train_op], feed)  #cost\n",
    "            losses['train'].append(train_loss)\n",
    "            train_summary_writer.add_summary(summaries, step)  #\n",
    "            \n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * (len(train_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    (len(train_X) // batch_size),\n",
    "                    train_loss))\n",
    "                \n",
    "        # Iteration using test data\n",
    "        for batch_i  in range(len(test_X) // batch_size):\n",
    "            x, y = next(test_batches)\n",
    "            \n",
    "            categories = np.zeros([batch_size, 18])\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6,1)[i]\n",
    "\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5,1)[i]\n",
    "\n",
    "            feed = {\n",
    "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
    "                movie_categories: categories,  #x.take(6,1)\n",
    "                movie_titles: titles,  #x.take(5,1)\n",
    "                targets: np.reshape(y, [batch_size, 1]),\n",
    "                dropout_keep_prob: 1,\n",
    "                lr: learning_rate}\n",
    "            \n",
    "            step, test_loss, summaries = sess.run([global_step, loss, inference_summary_op], feed)  #cost\n",
    "\n",
    "            # Save test loss\n",
    "            losses['test'].append(test_loss)\n",
    "            inference_summary_writer.add_summary(summaries, step)  #\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if (epoch_i * (len(test_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   test_loss = {:.3f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    (len(test_X) // batch_size),\n",
    "                    test_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver.save(sess, save_dir)  #, global_step=epoch_i\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save save_dir is used when generating forecasts.\n",
    "save_params((save_dir))\n",
    "load_dir = load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 251,
       "width": 376
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show training Loss\n",
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 377
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show test Loss\n",
    "# If the number of iterations increases again, the downward trend will be more obvious\n",
    "plt.plot(losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function get_tensor_by_name() to get tensors from loaded_graph, which will be used in the following recommended functions.\n",
    "\n",
    "def get_tensors(loaded_graph):\n",
    "\n",
    "    uid = loaded_graph.get_tensor_by_name(\"uid:0\")\n",
    "    user_gender = loaded_graph.get_tensor_by_name(\"user_gender:0\")\n",
    "    user_age = loaded_graph.get_tensor_by_name(\"user_age:0\")\n",
    "    user_job = loaded_graph.get_tensor_by_name(\"user_job:0\")\n",
    "    movie_id = loaded_graph.get_tensor_by_name(\"movie_id:0\")\n",
    "    movie_categories = loaded_graph.get_tensor_by_name(\"movie_categories:0\")\n",
    "    movie_titles = loaded_graph.get_tensor_by_name(\"movie_titles:0\")\n",
    "    targets = loaded_graph.get_tensor_by_name(\"targets:0\")\n",
    "    dropout_keep_prob = loaded_graph.get_tensor_by_name(\"dropout_keep_prob:0\")\n",
    "    lr = loaded_graph.get_tensor_by_name(\"LearningRate:0\")\n",
    "    \n",
    "    # The scheme of calculating the prediction score uses the name to obtain the tensor inference\n",
    "    inference = loaded_graph.get_tensor_by_name(\"inference/ExpandDims:0\")\n",
    "    movie_combine_layer_flat = loaded_graph.get_tensor_by_name(\"movie_fc/Reshape:0\")\n",
    "    user_combine_layer_flat = loaded_graph.get_tensor_by_name(\"user_fc/Reshape:0\")\n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference, movie_combine_layer_flat, user_combine_layer_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify users and movies to rate\n",
    "This part is to forward the network and calculate the predicted score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_movie(user_id_val, movie_id_val):\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "    \n",
    "        # Get Tensors from loaded model\n",
    "        uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference,_, __ = get_tensors(loaded_graph)  #loaded_graph\n",
    "    \n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = movies.values[movieid2idx[movie_id_val]][2]\n",
    "    \n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = movies.values[movieid2idx[movie_id_val]][1]\n",
    "    \n",
    "        feed = {\n",
    "              uid: np.reshape(users.values[user_id_val-1][0], [1, 1]),\n",
    "              user_gender: np.reshape(users.values[user_id_val-1][1], [1, 1]),\n",
    "              user_age: np.reshape(users.values[user_id_val-1][2], [1, 1]),\n",
    "              user_job: np.reshape(users.values[user_id_val-1][3], [1, 1]),\n",
    "              movie_id: np.reshape(movies.values[movieid2idx[movie_id_val]][0], [1, 1]),\n",
    "              movie_categories: categories,  #x.take(6,1)\n",
    "              movie_titles: titles,  #x.take(5,1)\n",
    "              dropout_keep_prob: 1}\n",
    "    \n",
    "        # Get Prediction\n",
    "        inference_val = sess.run([inference], feed)  \n",
    "    \n",
    "        return (inference_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\anaconda3\\envs\\665final\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./save\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[3.4801176]], dtype=float32)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_movie(234, 1401) #Predict user 234's rating on movie 1401"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Movie feature matrix\n",
    "Combine the trained movie features into a movie feature matrix and save it locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()  #\n",
    "movie_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:  #\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, movie_combine_layer_flat, __ = get_tensors(loaded_graph)  #loaded_graph\n",
    "\n",
    "    for item in movies.values:\n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = item.take(2)\n",
    "\n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = item.take(1)\n",
    "\n",
    "        feed = {\n",
    "            movie_id: np.reshape(item.take(0), [1, 1]),\n",
    "            movie_categories: categories,  #x.take(6,1)\n",
    "            movie_titles: titles,  #x.take(5,1)\n",
    "            dropout_keep_prob: 1}\n",
    "\n",
    "        movie_combine_layer_flat_val = sess.run([movie_combine_layer_flat], feed)  \n",
    "        movie_matrics.append(movie_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(movie_matrics).reshape(-1, 200)), open('movie_matrics.p', 'wb'))\n",
    "movie_matrics = pickle.load(open('movie_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_matrics = pickle.load(open('movie_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate User feature matrix\n",
    "Combine the trained user features into a user feature matrix and save it locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()  #\n",
    "users_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:  #\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, __,user_combine_layer_flat = get_tensors(loaded_graph)  #loaded_graph\n",
    "\n",
    "    for item in users.values:\n",
    "\n",
    "        feed = {\n",
    "            uid: np.reshape(item.take(0), [1, 1]),\n",
    "            user_gender: np.reshape(item.take(1), [1, 1]),\n",
    "            user_age: np.reshape(item.take(2), [1, 1]),\n",
    "            user_job: np.reshape(item.take(3), [1, 1]),\n",
    "            dropout_keep_prob: 1}\n",
    "\n",
    "        user_combine_layer_flat_val = sess.run([user_combine_layer_flat], feed)  \n",
    "        users_matrics.append(user_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(users_matrics).reshape(-1, 200)), open('users_matrics.p', 'wb'))\n",
    "users_matrics = pickle.load(open('users_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_matrics = pickle.load(open('users_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommending movies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_function import recommend_same_type_movie\n",
    "from models_function import recommend_your_favorite_movie\n",
    "from models_function import recommend_other_favorite_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recommend same type movie based on Items\n",
    "The idea is to calculate the cosine similarity between the feature vector of the current movie and the feature matrix of the entire movie, and take the top_k with the largest similarity. Here, some random selections are added to ensure that each recommendation is slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\renshangsi\\Desktop\\CS665\\Final\\Deep-Learning-Group-10\\models_function.py:240: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\renshangsi\\Desktop\\CS665\\Final\\Deep-Learning-Group-10\\models_function.py:242: The name tf.train.import_meta_graph is deprecated. Please use tf.compat.v1.train.import_meta_graph instead.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "The original movie is：[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "The following are recommendations：\n",
      "[1715 'Office Killer (1997)' 'Thriller']\n",
      "[2226 'Ring, The (1927)' 'Drama']\n",
      "[2486 '24-hour Woman (1998)' 'Drama']\n",
      "[219 'Cure, The (1995)' 'Drama']\n",
      "[3586 'Idolmaker, The (1980)' 'Drama']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{217, 1666, 2157, 2417, 3517}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_same_type_movie(1401, load_dir, movie_matrics, movies_orig, movieid2idx, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommend movies you like based on Users\n",
    "The idea is to use the user feature vector and the movie feature matrix to calculate the ratings of all movies, take the top_k with the highest rating, and also add some random selection parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "The following are recommendations：\n",
      "[922 'Sunset Blvd. (a.k.a. Sunset Boulevard) (1950)' 'Film-Noir']\n",
      "[858 'Godfather, The (1972)' 'Action|Crime|Drama']\n",
      "[904 'Rear Window (1954)' 'Mystery|Thriller']\n",
      "[2019\n",
      " 'Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954)'\n",
      " 'Action|Drama']\n",
      "[745 'Close Shave, A (1995)' 'Animation|Comedy|Thriller']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{735, 847, 892, 910, 1950}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_your_favorite_movie(234, load_dir, users_matrics, movie_matrics, movies_orig, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined recommend\n",
    "First, select top_k individuals who like a certain movie, and obtain the user feature vectors of these individuals.\n",
    "\n",
    "Then calculate the ratings of these people for all the movies\n",
    "\n",
    "Choose the movie with the highest rating for everyone as a recommendation\n",
    "\n",
    "Random selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "The original movie is：[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "The people who like to watch this movie are：[[5458 'F' 18 2]\n",
      " [1436 'F' 50 6]\n",
      " [3302 'M' 25 17]\n",
      " [1347 'F' 18 1]\n",
      " [2363 'M' 50 14]\n",
      " [5669 'M' 56 1]\n",
      " [2942 'F' 50 1]\n",
      " [4447 'M' 45 5]\n",
      " [1131 'M' 56 13]\n",
      " [3031 'M' 18 4]\n",
      " [355 'M' 18 3]\n",
      " [3018 'M' 35 0]\n",
      " [1731 'F' 18 0]\n",
      " [2158 'M' 25 12]\n",
      " [4085 'F' 25 6]\n",
      " [4696 'M' 18 12]\n",
      " [1644 'M' 18 12]\n",
      " [5254 'M' 18 4]\n",
      " [2338 'M' 45 17]\n",
      " [100 'M' 35 17]]\n",
      "People who like to watch this movie also like to watch：\n",
      "[912 'Casablanca (1942)' 'Drama|Romance|War']\n",
      "[37 'Across the Sea of Time (1995)' 'Documentary']\n",
      "[858 'Godfather, The (1972)' 'Action|Crime|Drama']\n",
      "[318 'Shawshank Redemption, The (1994)' 'Drama']\n",
      "[2019\n",
      " 'Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954)'\n",
      " 'Action|Drama']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{36, 315, 847, 900, 1950}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_other_favorite_movie(1401, load_dir, movie_matrics, movieid2idx, users_matrics, movies_orig, users_orig, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Understanding Convolutional Neural Networks for NLP\n",
    "http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n",
    "2. Convolutional Neural Networks for Sentence Classification\n",
    "https://github.com/yoonkim/CNN_sentence\n",
    "3. 利用TensorFlow实现卷积神经网络做文本分类\n",
    "https://www.jianshu.com/p/ed3eac3dcb39?from=singlemessage\n",
    "4.Convolutional Neural Network for Text Classification in Tensorflow \n",
    "https://github.com/dennybritz/cnn-text-classification-tf\n",
    "5.SVD Implement Recommendation systems\n",
    "https://github.com/songgc/TF-recomm\n",
    "6.Netflix电影推荐系统Python实现(协同过滤+矩阵分解)\n",
    "https://zhuanlan.zhihu.com/p/275873029\n",
    "7.TensorFlow实战——个性化推荐\n",
    "https://zhuanlan.zhihu.com/p/32078473"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
